---
title: "MB2 Merge eye-tracking data"
format: html
---

```{r}
library(tidyverse)
library(here)
library(assertthat)

source(here('helper','ensure_repo_structure.R'))
FIRST_TIME = FALSE
```
# Download and check columns

Download data locally

```{r download, eval= FIRST_TIME}
source(here('helper', 'osf_download.R'))
gather_osf_data("p3txj", XY_DATA_DIR)
```
Before we load these data, let's quickly check for compliance with column naming conventions. 

```{r validate columns}
cols <- c("lab_id", "participant_id", "media_name", 
          "x", "y", "t", "pupil_left", "pupil_right")

col_types = list(lab_id = col_character(),
                 participant_id = col_character(),
                 media_name = col_character(),
                 x = col_double(),
                 y = col_double(),
                 t = col_double(),
                 pupil_left = col_double(),
                 pupil_right = col_double())

local_files <- dir(here(XY_DATA_DIR), pattern = "*.csv")

for (f in local_files) {
  print(f)
  
  d <- read_csv(here(XY_DATA_DIR,f), n_max = 100, 
                col_types = col_types)
  
  # check that all columns are in the col list
  print(see_if(all(cols %in% names(d))))
  
  # check that no extra cols
  print(see_if(all(names(d) %in% cols)))
}
```

# Load local data

Now, load local data.

It is critical to downsample time to 40 Hz at this stage to ensure that we can load all the files effectively. Normally, this would go later in the pipeline but we can't load all the data without doing it. 

```{r load data}
source(here("helper","resampling_helper.R"))

xy <- local_files |>
  map_df(function(f) {
    print(f)
    
    d <- read_csv(here(XY_DATA_DIR,f),
                  col_types = col_types)
    
    
    d$age_cohort <- case_when(grepl('_adults_', f) ~ 'adults',
                              grepl('_toddlers_', f) ~ 'toddlers',
                              T ~ NA)
    
    d$participant_id <- paste(d$participant_id, d$age_cohort, sep = "-")
    
    
    # time resampling 
    # filter timepoints with NAs
    # we need to add numbers for media names so that we can preserve order
    # lab_event_num is an unvalidated event number just for the purpose of 
    # this resampling
    d <- d |>
      filter(!is.na(t), !is.na(media_name)) |>
      group_by(participant_id) |>
      mutate(lab_event_num = make_media_nums(media_name))
    
    # need to know they are in milliseconds
    is_microseconds <- median(diff(d$t)) > 500
    if (is_microseconds) {
      d$t <- d$t / 1000
    }
    
    # if in seconds
    is_seconds <- median(diff(d$t)) < .1
    if (is_seconds) {
      d$t <- d$t * 1000
    }
    
    # resample time to 40 Hz
    d_resamp <- d |>
      resample_times(timepoint_col_name = "t", 
                     trial_col_name = "lab_event_num") |>
      select(-lab_event_num)
    
    return(d_resamp)
  })

## TO DO: check on some residual warning messages generated during resampling

#save the intermediate
save(xy, file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_002A))
```

# Standardize media names

Remove file media extensions

```{r reload_resamps}
load(file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_002A))

xy <- xy %>% 
  mutate(media_name = tools::file_path_sans_ext(media_name))
```

Get an overview over media names used by different labs

```{r}
media_names_by_lab <- xy |>
  group_by(lab_id,media_name) |>
  count()
```


Begin standardizing data. The main thing we want to do here is validate the media names to make sure that we can use them for merge later. 

When there are invalid media names, you need to put them in the right `txt` files below. 

```{r standardize and validate data}
vec_renaming <- read_csv(here('metadata', 
                              'media_renaming.csv')) %>%
  {setNames(as.character(.$target), .$original)}

media_deletion <- readLines(here('metadata', 
                                 'media_names_to_remove.txt'))

media_names_valid <- readLines(here('metadata', 
                                    'media_names_validate.txt'))

data <- xy |>
  filter(!is.na(media_name) & !(media_name %in% media_deletion)) %>% 
  mutate(media_name = ifelse(media_name %in% names(vec_renaming), vec_renaming[as.character(media_name)], media_name)) %>% 
  group_by(lab_id, participant_id) %>% 
  mutate(event_num = cumsum(c(1, na.omit(media_name != lag(media_name))))) %>% 
  ungroup()
```

Check that all media names fit the appropriate schema. 

```{r checking_names}

# quick fix to push data further down the pipeline while excluding broken names
#data <- data %>% filter(media_name %in% media_names_valid) 

# this object helps to determine where invalid media names are coming from in cases where the fix is non-obvious
invalid_media_names <- data %>% 
  distinct(lab_id, participant_id, media_name) %>% 
  filter(!media_name %in% media_names_valid)

invalid_media_names$media_name
unique(invalid_media_names$media_name)

assert_that(nrow(invalid_media_names) == 0)
rm(invalid_media_names)
```


# Column-wise validation

We have: 

```{r}
names(data)
```

Let's go column by column. 

## lab_id

First, let's canonicalize the lab_ids, then check that they match demographics.

```{r}
lab_ids <- read_csv(here("metadata","labids.csv")) |>
  rename(lab_id = LabID) 

data_labfix <- fuzzyjoin::stringdist_left_join(data, lab_ids, 
                                               by = "lab_id", 
                                               max_dist = 1, 
                                               distance_col = "distance",
                                               ignore_case = TRUE) |>
  mutate(lab_id.y = case_when(lab_id.x == "oxfordBabylab" ~ "babylabOxford",
                              lab_id.x == "PLUS" ~ "ToMcdlSalzburg",
                              TRUE ~ lab_id.y)) |>
  rename(lab_id = lab_id.y)  

assert_that(!any(is.na(data_labfix$lab_id)))
```

Now check for match to demos. 


```{r}
load(file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_001_ADULT))
load(file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_001_TODDLER))

```

```{r}

et_adult_labids <- unique(filter(data_labfix, 
                                 age_cohort == "adults")$lab_id)
et_toddler_labids <- unique(filter(data_labfix, 
                                   age_cohort == "toddlers")$lab_id)

demo_adult_labids <- unique(adult_demo$lab_id)
demo_toddler_labids <- unique(toddler_demo$lab_id)
```

Check overlap/non-overlap in IDs. First adults.

```{r}
print(paste("demo IDs not in ET:", 
            dplyr::setdiff(demo_adult_labids, et_adult_labids)))

print(paste("ET IDs not in demo:", 
            dplyr::setdiff(et_adult_labids, demo_adult_labids)))
```
Now toddlers. 

```{r}
print(paste("demo IDs not in ET:", 
            dplyr::setdiff(demo_toddler_labids, et_toddler_labids)))

print(paste("ET IDs not in demo:", 
            dplyr::setdiff(et_toddler_labids, demo_toddler_labids)))
```

Now the test.

```{r}
assert_that(is_empty(dplyr::setdiff(demo_adult_labids, et_adult_labids)))
assert_that(is_empty(dplyr::setdiff(et_adult_labids, demo_adult_labids)))
assert_that(is_empty(dplyr::setdiff(demo_toddler_labids, et_toddler_labids)))
assert_that(is_empty(dplyr::setdiff(et_toddler_labids, demo_toddler_labids)))

```
```{r}
data <- data_labfix |>
  select(-lab_id.x, distance)
```

## participant_id

```{r}
correct_ids <- function(df) {
  renaming <- read.csv2(here('metadata','participant_id_renaming.csv')) %>% 
    mutate(
      sample = paste0(sample,'s'),
      old = paste0(old,'-',sample),
      new = paste0(new,'-',sample)
      )
  
  # Merge the input dataframe with the renaming dataframe
  df <- df %>%
    left_join(renaming, by = c("lab_id" = "lab", "age_cohort" = "sample", "participant_id" = "old"))
  
  # Replace the participant_id with the correct id
  df <- df %>%
    mutate(participant_id = ifelse(is.na(new), participant_id, new)) %>%
    select(-new)
  
  # Additional cleanup
  df <- df %>%
    mutate(participant_id = case_when(
      lab_id == 'gertlabLancaster' ~ paste0("LU_", participant_id),
      age_cohort == 'toddlers' & lab_id == 'irlConcordia' ~ sub("MB2_", "", participant_id),
      age_cohort == 'adults' & lab_id == 'careylabHarvard' ~ paste0('careylabHarvard_', participant_id),
      age_cohort == 'adults' & lab_id == 'MEyeLab' ~ sub("\\.edf", "", participant_id),
      age_cohort == 'toddlers' & lab_id == 'careylabHarvard' ~ sub("careylabHarvard", "careylabharard", participant_id),
      lab_id == 'UIUCinfantlab' ~ paste0("0", substr(participant_id, 1, 2), "-", age_cohort),
      TRUE ~ participant_id
    ))

  return(df)
}

data <- correct_ids(data)

# remove ghost ids that are not found in the demographic data
data <- data %>%
  filter(!(lab_id == 'beinghumanWroclaw' & participant_id %in% c('ADULT_006-adults', 'BABY_010-toddlers')))

```

```{r}
et_adult_pids <- filter(data, age_cohort == "adults") |>
  select(lab_id, participant_id) |>
  distinct() 

et_toddler_pids <- filter(data, age_cohort == "toddlers") |>
  select(lab_id, participant_id) |>
  distinct() 

demo_adult_pids <- adult_demo |>
  select(lab_id, participant_id) |>
  distinct() 

demo_toddler_pids <- toddler_demo |>
  select(lab_id, participant_id) |>
  distinct()
```

Check overlap/non-overlap in IDs. First adults.

```{r}
metadata <- read.csv(here("metadata","known_missing.csv"))

# Filter for adults
metadata_adults <- metadata %>% filter(age_cohort == "adult") %>% mutate(participant_id = paste0(participant_id,'-adults'))

# Remove matches from et_adult_pids
et_ids_not_in_demo_adults <- anti_join(et_adult_pids, demo_adult_pids) %>% 
  anti_join(metadata_adults, by = c("lab_id", "participant_id"))

# Remove matches from demo_adult_pids
demo_ids_not_in_et_adults <- anti_join(demo_adult_pids, et_adult_pids) %>% 
  anti_join(metadata_adults, by = c("lab_id", "participant_id"))

#FILTER_LAB <- 'WSUMARCS'
#et_ids_not_in_demo_adults %>% rename(et_id = participant_id) %>% filter(lab_id == FILTER_LAB)
#demo_ids_not_in_et_adults %>% rename(demo_id = participant_id) %>%filter(lab_id == FILTER_LAB)

unique(c(et_ids_not_in_demo_adults$lab_id, demo_ids_not_in_et_adults$lab_id))
```

Now toddlers. 

```{r}
metadata <- read.csv(here("metadata","known_missing.csv"))

metadata_toddlers <- metadata %>% filter(age_cohort == "toddler") %>% mutate(participant_id = paste0(participant_id,'-toddlers'))

# Remove matches from et_toddler_pids
et_ids_not_in_demo_toddlers <- anti_join(et_toddler_pids, demo_toddler_pids) %>%
  anti_join(metadata_toddlers, by = c("lab_id", "participant_id"))

# Remove matches from demo_toddler_pids
demo_ids_not_in_et_toddlers <- anti_join(demo_toddler_pids, et_toddler_pids) %>%
  anti_join(metadata_toddlers, by = c("lab_id", "participant_id"))

#FILTER_LAB <- 'gertlabLancaster'
#et_ids_not_in_demo_toddlers %>% rename(et_id = participant_id) %>% filter(lab_id == FILTER_LAB)
#demo_ids_not_in_et_toddlers %>% rename(demo_id = participant_id) %>%filter(lab_id == FILTER_LAB)

#unique(c(et_ids_not_in_demo_toddlers$lab_id, demo_ids_not_in_et_toddlers$lab_id))
```

Now the test.

```{r}
assert_that(is_empty(anti_join(et_adult_pids, demo_adult_pids)))
#assert_that(is_empty(anti_join(demo_adult_pids, et_adult_pids))) # unfixable, as data is missing
assert_that(is_empty(anti_join(et_toddler_pids, demo_toddler_pids)))
#assert_that(is_empty(anti_join(demo_toddler_pids, et_toddler_pids))) # unfixable, as data is missing
```


## x


```{r}
data |>
  group_by(lab_id) |>
  summarise(min_x = min(x, na.rm=TRUE), 
            max_x = max(x, na.rm=TRUE))
```

Follow up on x distribution for outlier labs. 

```{r}
data |>
  filter(lab_id == "oxfordBabylab") |>
  ggplot(aes(x = x)) + 
  geom_histogram()

data |>
  filter(lab_id == "babylabNijmegen") |>
  ggplot(aes(x = x)) + 
  geom_histogram()

```

Tests.

```{r}
assert_that(all(data$x > 0))
assert_that(all(data$x < 5000))
```


## y

Lots of very negative data. Not sure what to do with this. 

```{r}
data |>
  group_by(lab_id) |>
  summarise(min_x = min(x, na.rm=TRUE), 
            max_x = max(x, na.rm=TRUE))
```

Tests.

```{r}
assert_that(all(data$y > 0))
assert_that(all(data$y < 5000))
```

## t

It's pretty clear that some datasets are in seconds and some are in milliseconds.

```{r}
data |>
  group_by(lab_id, participant_id) |>
  mutate(t_norm = t - t[1]) |>
  group_by(lab_id) |>
  summarise(min_t = min(t_norm, na.rm=TRUE), 
            max_t = max(t_norm, na.rm=TRUE), 
            mean_diff = median(diff(t_norm), na.rm=TRUE))

```


## pupil_left & pupil_right

```{r}
data |>
  group_by(lab_id) |>
  summarise(min_pupil_left = min(pupil_left, na.rm=TRUE),
            max_pupil_left = max(pupil_left, na.rm=TRUE), 
            min_pupil_right = min(pupil_right, na.rm=TRUE),
            max_pupil_right = max(pupil_right, na.rm=TRUE)) 

```


## age_cohort

```{r}
assert_that(all(data$age_cohort %in% c("adults","toddlers")))
```


## event_num

Why are there some event numbers that go to 26?

```{r}
unique(data$event_num)
```


```{r}
hist(data$event_num, breaks = 0:26)
```


Why are there so many instances of event 7?

```{r}
data |>
  group_by(lab_id, age_cohort, event_num) |>
  count()

```

# Clean demographics

Clean up demographics. 

```{r}
# remove this for now but can bring it back if we need to simplify the demographic information that is retained

# adult_demo_minimal <- adult_demo |>
#   mutate(age_cohort = "adults") |>
#   select(participant_id, lab_id, age_cohort, method, pilot, 
#          session_error, fam1_error, fam2_error, fam3_error, fam4_error, 
#          test1_error, test2_error, 
#          participant_gender, 
#          hearing, vision, medical_issues, 
#          contains("native_lang"), second_session, age_years_n, country)
# 
# toddler_demo_minimal <- toddler_demo |>
#   mutate(age_cohort = "toddlers") |>
#   select(participant_id, lab_id, age_cohort, method, pilot, 
#          session_error, fam1_error, fam2_error, fam3_error, fam4_error, 
#          test1_error, test2_error, 
#          participant_gender, 
#          hearing, vision, medical_issues, 
#          contains("native_lang"), second_session, age_days, country)

#bind together adult and toddler demographics
#necessary in order to avoid issues with the merge
combined_demo <- adult_demo |>
  mutate(test_order = as.character(test_order)) |>
  mutate(age_cohort="adults") %>% #just to make sure that nothing goes wrong with the merge
  bind_rows(
    toddler_demo |>
      mutate(test_order = as.character(test_order)) |>
      mutate(age_cohort="toddlers")
    )
                         
```

# Exclusions

## Age Exclusions

First, process age information in the demographic file
```{r}
hist(combined_demo$age_days_num)

combined_demo <- combined_demo |>
  mutate(age_mo = case_when(
    #check age conversion we want to use here
    age_cohort=="toddlers" ~ age_days_num / 365.25*12,
    TRUE ~ NA_real_))

#quick checks
hist(combined_demo$age_mo)
hist(combined_demo$age_years_n)
table(filter(combined_demo,age_cohort=="adults")$age_years_n)
```

Now add columns tracking exclusions based on age. 

```{r}
combined_demo <- combined_demo |>
  mutate(
    age_exclusion = case_when(
      age_cohort == "adults" & age_years_n < 18 ~ "yes",
      age_cohort == "adults" & age_years_n > 55 ~ "yes",
      age_cohort == "adults" & is.na(age_years_n) ~ NA_character_,
      age_cohort == "toddlers" & age_mo < 18 ~ "yes",
      age_cohort == "toddlers" & age_mo >= 28 ~ "yes",
      age_cohort == "toddlers" & is.na(age_mo) ~ NA_character_,
      TRUE ~ "no"
    )
  )
View(select(combined_demo,age_cohort,age_years_n,age_mo,age_exclusion))
```

# Session Exclusions

Breakdown of session errors.

```{r}
#interim breakdown to see intersection of both session errors and age exlusion
exclusion_summary <- combined_demo |> 
  filter(session_error=="error"|age_exclusion=="yes") |> 
  select(age_cohort,participant_id,lab_id,session_error,session_error_info,age_exclusion) |>
  group_by(age_exclusion,session_error,session_error_info) |> 
  summarize(
    n=n(),
    )
```

# Merge

Merge all the data with the demographics.

```{r}
data_merged <- left_join(data, combined_demo, 
                         by = c("age_cohort","lab_id","participant_id"))
```

# Saving 

Now save the merged xy data locally.

```{r saving}
save(data, file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_002))
save(data_merged, file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_002B))
```
