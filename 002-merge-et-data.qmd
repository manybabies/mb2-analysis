---
title: "Merge eye-tracking data"
format: html
---

```{r}
library(tidyverse)
library(here)
library(assertthat)
source(here("helper","resampling_helper.R"))
FIRST_TIME = FALSE
```

Download data locally

```{r download, eval= FIRST_TIME}

mb2_data <- osfr::osf_retrieve_node("p3txj")
files <- osfr::osf_ls_files(mb2_data)

PROCESSED_DATA_DIR = "processed_xy_data"

dir.create(here(PROCESSED_DATA_DIR), showWarnings = FALSE)

files |>
  mutate(idx = 1:n()) %>%
  base::split(.$idx) |>
  map(function(f) {
    print(f)
    osfr::osf_download(f, path = here(PROCESSED_DATA_DIR))
  })
```

Load local data

```{r}
cols <- c("lab_id", "participant_id", "media_name", 
          "x", "y", "t", "pupil_left", "pupil_right")

local_files <- dir(here("processed_xy_data"))

xy <- local_files |>
  map_df(function(x) {
    print(x)
    d <- read_csv(here("processed_xy_data",x),
                  col_types = list(
                    lab_id = col_character(),
                    participant_id = col_character(),
                    media_name = col_character(),
                    x = col_double(),
                    y = col_double(),
                    t = col_double(),
                    pupil_left = col_double(),
                    pupil_right = col_double()
                  ))
    
    # check that all columns are in the col list
    assert_that(all(cols %in% names(d)))
    
    # check that no extra cols
    assert_that(all(names(d) %in% cols))
    
    return(d)
  })
```
## Processing tasks that we will do centrally

Reconcile media names - Code done, renamings for new datasets still pending
Create trial numbers - done
Standardize media names - Code done, renamings for new datasets still pending
Zero times within trials - done
Resample times - done

Flip coordinate origin
Clip XY outside of screen coordinates
Create/process AOIs
Standardize pupil sizes




```{r standardize and validate data}

vec_renaming <- read_csv(here('metadata','media_renaming.csv')) %>%
  {setNames(as.character(.$target), .$original)}
media_deletion <- readLines(here('metadata','media_names_to_remove.txt'))
media_names_valid <- readLines(here('metadata','media_names_validate.txt'))

data <- xy |>
  mutate(media_name = tools::file_path_sans_ext(media_name)) %>%
  filter(!is.na(media_name) & !(media_name %in% media_deletion)) %>% 
  mutate(media_name = ifelse(media_name %in% names(vec_renaming), vec_renaming[as.character(media_name)], media_name)) %>% 
  group_by(lab_id, participant_id) %>% 
  mutate(event_num = cumsum(c(1, na.omit(media_name != lag(media_name))))) %>% 
  ungroup()

  
# Validate media names
invalid_names <- setdiff(unique(data$media_name), media_names_valid)
# TODO check for new datasets, update the renaming/deletion file accordingly

# Extract media version information from media version string
data <- data %>% mutate(
  media_version = ifelse(grepl('_new', media_name),0,1),
  media_name = gsub("_new", "", media_name))

# Add trial numbers to the data
trial_orders <- data %>% 
  filter(media_name %in% media_names_valid) %>%
  group_by(lab_id, participant_id) %>%
  mutate(trial_num = cumsum(c(1, na.omit(media_name != lag(media_name))))) %>%
  distinct(lab_id, participant_id, media_name, trial_num)

data <- data %>% left_join(trial_orders, by = join_by(lab_id, participant_id, media_name))


# Validate that the (lab specific) participant ids in the data line up with the ids from the (lab specific) demographic files
# This also triggers on mismatches in labid namings
id_mismatches <- trial_orders %>% 
  distinct(lab_id, participant_id) %>% 
  mutate(xy_exists = T) %>%
  full_join(adult_demo, by=c('lab_id' = 'labid', 'participant_id')) %>% 
  filter(is.na(xy_exists) | is.na(testing_date))
#assert_that(nrow(id_mismatches) == 0)
# TODO Have a close look at this once all of the data was collected

# Validate trial orders
trial_orders_wide <- trial_orders %>%
  pivot_wider(id_cols = c('lab_id', 'participant_id'), values_from=media_name, names_from=trial_num, names_prefix='trial_')

trial_orders_design <- read.csv(here('helper','trial_order.csv')) %>% 
  left_join(read.csv(here('helper','fam_order.csv')), by=join_by(fam_order))

invalid_trial_orders <- trial_orders_wide %>% 
  anti_join(trial_orders_design, by=paste0('trial_',1:6))
#assert_that(nrow(invalid_trial_orders) == 0)

trial_order_mismatches <- adult_demo %>% # TODO: This only includes adult data right now as the demographic script is still WIP
  select(labid, participant_id, test_order) %>% 
  inner_join(trial_orders_wide %>% 
               inner_join(trial_orders_design, by=paste0('trial_',1:6)) # determine seen trials
    , by=c('labid' = 'lab_id', 'participant_id')) %>% 
  filter(test_order != trial_order)

assert_that(nrow(trial_order_mismatches) == 0)
rm(trial_orders, trial_orders_design)
```

Rezero and resample times


```{r}
#filter timepoints with NAs
data <- data %>%
  filter(!is.na(t))

#filter data without associated events
data <- data %>%
  filter(!is.na(event_num))

#rezero time
data_rezeroed <- data %>%
  rezero_times() # right now, rezeroing on event_num (NOT trial_num)

#"normalize" time according to a point of disambiguation
#normalizing isn't really relevant here (I don't think) but preserving for now in case we want to include any time normalization
data_normalized <- data_rezeroed %>%
  mutate(point_of_disambiguation=0) %>% 
  normalize_times()

# resample time to 40 Hz
data_resampled <- data_normalized %>%
  resample_times()

#Validate that time is provided in milliseconds
#assert_that(mean(diff(data_normalized$t_norm),na.rm=TRUE) > 1 & mean(diff(data_normalized$t_norm), na.rm=TRUE) < 100)
# TODO this still fails, have a closer look

```

```{r}

# Adrian (centered in screen, not fullscreen): 
# TODO: Flip Coordinate Origin
# TODO: XY Trimming
# TODO: XY -> Trimming, import the screensizes from the google sheet

# TODO: Visualize this data to double check?

```


Issues to deal with:

- Repo needs some serious cleanup - hard to keep track of what files contain relevant code and what is not needed anymore
- osf download still fails (?)
- Adults and toddlers share most (all) of the pipeline but have differing demo files - combine into one dataset and split later for analysis?

- some timepoints are not being read in correctly (e.g. in careylabHarvard_adults_xy_timepoints), apparently because these are too large (10^12) - Martin: I think I fixed this now by removing invariant leading digits (on OSF)
- some residual import/ processing issues with some datasets
-- careylabHarvard_adults_xy_timepoints has some participant ids under lab_id - Martin: this is now fixed on OSF
- having some trouble pulling the current version of the data from OSF 
-resampling pupil size??
- really need to be cautious about the definition of trials/ events/ etc. - currently fragile 
- passing the final/correct column names to resample_xy_trial

- How to standardize pupil sizes?
- Web-based eyetracking will need special treatment at specific points of the pipeline (Adrian: Think about this some more)



