---
title: "Merge eye-tracking data"
format: html
---

```{r}
library(tidyverse)
library(here)
library(assertthat)
source(here("helper","resampling_helper.R"))
FIRST_TIME = FALSE
```

Download data locally

```{r download, eval= FIRST_TIME}

mb2_data <- osfr::osf_retrieve_node("p3txj")
files <- osfr::osf_ls_files(mb2_data)

PROCESSED_DATA_DIR = "processed_xy_data"

dir.create(here(PROCESSED_DATA_DIR), showWarnings = FALSE)

files |>
  mutate(idx = 1:n()) %>%
  base::split(.$idx) |>
  map(function(f) {
    print(f)
    osfr::osf_download(f, path = here(PROCESSED_DATA_DIR))
  })
```

Load local data

```{r}
cols <- c("lab_id", "participant_id", "media_name", 
          "x", "y", "t", "pupil_left", "pupil_right")

local_files <- dir(here("processed_xy_data"))

xy <- local_files |>
  map_df(function(x) {
    print(x)
    d <- read_csv(here("processed_xy_data",x),
                  col_types = list(
                    lab_id = col_character(),
                    participant_id = col_character(),
                    media_name = col_character(),
                    x = col_double(),
                    y = col_double(),
                    t = col_double(),
                    pupil_left = col_double(),
                    pupil_right = col_double()
                  ))
    
    # check that all columns are in the col list
    assert_that(all(cols %in% names(d)))
    
    # check that no extra cols
    assert_that(all(names(d) %in% cols))
    
    return(d)
  })
```
## Processing tasks that we will do centrally

Reconcile media names - Code done, renamings for new datasets still pending
Create trial numbers - Done
Standardize media names - Code done, renamings for new datasets still pending
Zero times within trials - Martin - done
Resample times - Marin
Clip XY outside of screen coordinates - Adrian
Create/process AOIs - Adrian
Standardize pupil sizes - ?



```{r standardize and validate data}

vec_renaming <- read_csv(here('metadata','media_renaming.csv')) %>%
  {setNames(as.character(.$target), .$original)}
media_deletion <- readLines(here('metadata','media_names_to_remove.txt'))
media_names_valid <- readLines(here('metadata','media_names_validate.txt'))

data <- xy |>
  mutate(media_name = tools::file_path_sans_ext(media_name)) %>%
  filter(!is.na(media_name) & !(media_name %in% media_deletion)) %>% 
  mutate(media_name = ifelse(media_name %in% names(vec_renaming), vec_renaming[as.character(media_name)], media_name)) %>% 
  group_by(lab_id, participant_id) %>% 
  mutate(media_index = cumsum(c(1, na.omit(media_name != lag(media_name))))) %>% 
  ungroup()


# Validate that the participant ids in the data line up with the ids from the demographic files
# TODO (Adrian)
  
# Validate media names
invalid_names <- setdiff(unique(data$media_name), media_names_valid)
# TODO check for new datasets, update the renaming/deletion file accordingly

# Extract media version information from media version string
data <- data %>% mutate(
  media_version = ifelse(grepl('_new', media_name),0,1),
  media_name = gsub("_new", "", media_name))

# Add trial numbers to the data
trial_orders <- data %>% 
  filter(media_name %in% media_names_valid) %>%
  group_by(lab_id, participant_id) %>%
  mutate(trial_num = cumsum(c(1, na.omit(media_name != lag(media_name))))) %>%
  distinct(lab_id, participant_id, media_name, trial_num)

data <- data %>% left_join(trial_orders, by = join_by(lab_id, participant_id, media_name))

# Add event numbers to the data (just in case we need to distinguish this from trial numbers)
data <- data %>% 
  group_by(lab_id, participant_id) %>%
  mutate(event_num = cumsum(c(1, na.omit(media_name != lag(media_name)))))

# Validate trial order
# TODO: get ground truth and demographic information in here for comparison (Adrian)

rm(trial_orders)
```

Rezero and resample times


```{r}
#filter timepoints with NAs
data <- data %>%
  filter(!is.na(t))

#filter data without associated events
data <- data %>%
  filter(!is.na(event_num))

#rezero time
data_rezeroed <- data %>%
  rezero_times() # right now, rezeroing on event_num (NOT trial_num)

#"normalize" time according to a point of disambiguation
#normalizing isn't really relevant here (I don't think) but preserving for now in case we want to include any time normalization
data_normalized <- data_rezeroed %>%
  mutate(point_of_disambiguation=0) %>% 
  normalize_times()

# resample time to 40 Hz
data_resampled <- data_normalized %>%
  resample_times()
  
```


Issues to deal with:
- some timepoints are not being read in correctly (e.g. in careylabHarvard_adults_xy_timepoints), apparently because these are too large (10^12) - Martin: I think I fixed this now by removing invariant leading digits (on OSF)
- some residual import/ processing issues with some datasets
-- careylabHarvard_adults_xy_timepoints has some participant ids under lab_id - Martin: this is now fixed on OSF
- having some trouble pulling the current version of the data from OSF 
-resampling pupil size??
- really need to be cautious about the definition of trials/ events/ etc. - currently fragile 
- passing the final/correct column names to resample_xy_trial


```{r}
# Validate that the scale of timesteps is correct
# TODO once martin has taken care of the timepoints (Martin)
    # Mikes previous code: #assert_that(mean(diff(d$t),na.rm=TRUE) > 1 & mean(diff(d$t), na.rm=TRUE) < 100)
```

