---
title: "MB2 Merge eye-tracking data"
format: html
---

```{r}
library(tidyverse)
library(here)
library(assertthat)

source(here('helper','ensure_repo_structure.R'))
FIRST_TIME = FALSE
```
# Download and check columns

Download data locally

```{r download, eval= FIRST_TIME}
source(here('helper', 'osf_download.R'))
gather_osf_data("p3txj", XY_DATA_DIR)
```
Before we load these data, let's quickly check for compliance with column naming conventions. 

```{r validate columns}
cols <- c("lab_id", "participant_id", "media_name", 
          "x", "y", "t", "pupil_left", "pupil_right")

col_types = list(lab_id = col_character(),
                 participant_id = col_character(),
                 media_name = col_character(),
                 x = col_double(),
                 y = col_double(),
                 t = col_double(),
                 pupil_left = col_double(),
                 pupil_right = col_double())

local_files <- dir(here(XY_DATA_DIR), pattern = "*.csv")

for (f in local_files) {
  print(f)
  
  d <- read_csv(here(XY_DATA_DIR,f), n_max = 100, 
                col_types = col_types)
  
  # check that all columns are in the col list
  print(see_if(all(cols %in% names(d))))
  
  # check that no extra cols
  print(see_if(all(names(d) %in% cols)))
}
```

# Load local data

Now, load local data.

It is critical to downsample time to 40 Hz at this stage to ensure that we can load all the files effectively. Normally, this would go later in the pipeline but we can't load all the data without doing it. 

```{r load data}
source(here("helper","resampling_helper.R"))

xy <- local_files |>
  map_df(function(f) {
    print(f)
    
    d <- read_csv(here(XY_DATA_DIR,f),
                  col_types = col_types)
    
    
    d$age_cohort <- case_when(grepl('_adults_', f) ~ 'adults',
                              grepl('_toddlers_', f) ~ 'toddlers',
                              T ~ NA)
    
    d$participant_id <- paste(d$participant_id, d$age_cohort, sep = "_")
    
    
    # time resampling 
    # filter timepoints with NAs
    # we need to add numbers for media names so that we can preserve order
    # lab_event_num is an unvalidated event number just for the purpose of 
    # this resampling
    d <- d |>
      filter(!is.na(t), !is.na(media_name)) |>
      group_by(participant_id) |>
      mutate(lab_event_num = make_media_nums(media_name))

    # need to know they are in milliseconds
    is_microseconds <- median(diff(d$t)) > 500
    if (is_microseconds) {
      d$t <- d$t / 1000
    }
    
    # if in seconds
    is_seconds <- median(diff(d$t)) < .1
    if (is_seconds) {
      d$t <- d$t * 1000
    }
    
    # resample time to 40 Hz
    d_resamp <- d |>
      resample_times(timepoint_col_name = "t", 
                     trial_col_name = "lab_event_num") |>
      select(-lab_event_num)
    
    return(d_resamp)
  })

## TO DO: check on some residual warning messages generated during resampling

#save the intermediate
save(xy, file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_002A))
```

# Standardize media names

Remove file media extensions

```{r reload_resamps}
load(file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_002A))

xy <- xy %>% 
  mutate(media_name = tools::file_path_sans_ext(media_name))
```

Get an overview over media names used by different labs

```{r}
media_names_by_lab <- xy |>
  group_by(lab_id,media_name) |>
  count()
```


Begin standardizing data. The main thing we want to do here is validate the media names to make sure that we can use them for merge later. 

When there are invalid media names, you need to put them in the right `txt` files below. 

```{r standardize and validate data}
vec_renaming <- read_csv(here('metadata', 
                              'media_renaming.csv')) %>%
  {setNames(as.character(.$target), .$original)}

media_deletion <- readLines(here('metadata', 
                                 'media_names_to_remove.txt'))

media_names_valid <- readLines(here('metadata', 
                                    'media_names_validate.txt'))

data <- xy |>
  filter(!is.na(media_name) & !(media_name %in% media_deletion)) %>% 
  mutate(media_name = ifelse(media_name %in% names(vec_renaming), vec_renaming[as.character(media_name)], media_name)) %>% 
  group_by(lab_id, participant_id) %>% 
  mutate(event_num = cumsum(c(1, na.omit(media_name != lag(media_name))))) %>% 
  ungroup()
```

Check that all media names fit the appropriate schema. 

```{r checking_names}

# quick fix to push data further down the pipeline while excluding broken names
#data <- data %>% filter(media_name %in% media_names_valid) 

# this object helps to determine where invalid media names are coming from in cases where the fix is non-obvious
invalid_media_names <- data %>% 
  distinct(lab_id, participant_id, media_name) %>% 
  filter(!media_name %in% media_names_valid)

invalid_media_names$media_name
unique(invalid_media_names$media_name)

assert_that(nrow(invalid_media_names) == 0)
rm(invalid_media_names)
```


# Column-wise validation

We have: 

```{r}
names(data)
```

Let's go column by column. 

## lab_id

First, let's canonicalize the lab_ids, then check that they match demographics.

```{r}
lab_ids <- read_csv(here("metadata","labids.csv")) |>
  rename(lab_id = LabID) 

data_labfix <- fuzzyjoin::stringdist_left_join(data, lab_ids, 
                by = "lab_id", 
                max_dist = 1, 
                distance_col = "distance",
                ignore_case = TRUE) |>
  mutate(lab_id.y = case_when(lab_id.x == "oxfordBabylab" ~ "babylabOxford",
                              lab_id.x == "PLUS" ~ "ToMcdlSalzburg",
                              TRUE ~ lab_id.y)) |>
  rename(lab_id = lab_id.y)  

assert_that(!any(is.na(data_labfix$lab_id)))
```

Now check for match to demos. 


```{r}
load(file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_001_ADULT))
load(file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_001_TODDLER))

et_adult_labids <- unique(filter(data_labfix, 
                                 age_cohort == "adults")$lab_id)
et_toddler_labids <- unique(filter(data_labfix, 
                                   age_cohort == "toddlers")$lab_id)

demo_adult_labids <- unique(adult_demo$lab_id)
demo_toddler_labids <- unique(toddler_demo$lab_id)
```

Check overlap/non-overlap in IDs. First adults.

```{r}
print(paste("demo IDs not in ET:", 
            dplyr::setdiff(demo_adult_labids, et_adult_labids)))

print(paste("ET IDs not in demo:", 
            dplyr::setdiff(et_adult_labids, demo_adult_labids)))
```
Now toddlers. 

```{r}
print(paste("demo IDs not in ET:", 
            dplyr::setdiff(demo_toddler_labids, et_toddler_labids)))

print(paste("ET IDs not in demo:", 
            dplyr::setdiff(et_toddler_labids, demo_toddler_labids)))
```

Now the test.

```{r}
assert_that(is_empty(dplyr::setdiff(demo_adult_labids, et_adult_labids)))
assert_that(is_empty(dplyr::setdiff(et_adult_labids, demo_adult_labids)))
assert_that(is_empty(dplyr::setdiff(demo_toddler_labids, et_toddler_labids)))
assert_that(is_empty(dplyr::setdiff(et_toddler_labids, demo_toddler_labids)))

```
```{r}
data <- data_labfix |>
  select(-lab_id.x, distance)
```

## participant_id


```{r}
et_adult_pids <- filter(data, age_cohort == "adults") |>
  select(lab_id, participant_id) |>
  distinct() 

et_toddler_pids <- filter(data, age_cohort == "toddlers") |>
  select(lab_id, participant_id) |>
  distinct() 

demo_adult_pids <- adult_demo |>
  select(lab_id, participant_id) |>
  distinct() 

demo_toddler_pids <- toddler_demo |>
  select(lab_id, participant_id) |>
  distinct()
```

Check overlap/non-overlap in IDs. First adults.

```{r}
anti_join(et_adult_pids, demo_adult_pids)

anti_join(demo_adult_pids, et_adult_pids)
```

Now toddlers. 

```{r}
anti_join(et_adult_pids, demo_adult_pids)

anti_join(demo_adult_pids, et_adult_pids)
```

Now the test.

```{r}
assert_that(is_empty(anti_join(et_adult_pids, demo_adult_pids)))
assert_that(is_empty(anti_join(demo_adult_pids, et_adult_pids)))
assert_that(is_empty(anti_join(et_adult_pids, demo_adult_pids)))
assert_that(is_empty(anti_join(demo_adult_pids, et_adult_pids)))
```


## x


```{r}
data |>
  group_by(lab_id) |>
  summarise(min_x = min(x, na.rm=TRUE), 
            max_x = max(x, na.rm=TRUE))
```

Follow up on x distribution for outlier labs. 

```{r}
data |>
  filter(lab_id == "oxfordBabylab") |>
  ggplot(aes(x = x)) + 
  geom_histogram()

data |>
  filter(lab_id == "babylabNijmegen") |>
  ggplot(aes(x = x)) + 
  geom_histogram()

```

Tests.

```{r}
assert_that(all(data$x > 0))
assert_that(all(data$x < 5000))
```


## y

Lots of very negative data. Not sure what to do with this. 

```{r}
data |>
  group_by(lab_id) |>
  summarise(min_x = min(x, na.rm=TRUE), 
            max_x = max(x, na.rm=TRUE))
```

Tests.

```{r}
assert_that(all(data$y > 0))
assert_that(all(data$y < 5000))
```

## t

It's pretty clear that some datasets are in seconds and some are in milliseconds.

```{r}
data |>
  group_by(lab_id, participant_id) |>
  mutate(t_norm = t - t[1]) |>
  group_by(lab_id) |>
  summarise(min_t = min(t_norm, na.rm=TRUE), 
            max_t = max(t_norm, na.rm=TRUE), 
            mean_diff = median(diff(t_norm), na.rm=TRUE))

```


## pupil_left & pupil_right

```{r}
data |>
  group_by(lab_id) |>
  summarise(min_pupil_left = min(pupil_left, na.rm=TRUE),
            max_pupil_left = max(pupil_left, na.rm=TRUE), 
            min_pupil_right = min(pupil_right, na.rm=TRUE),
            max_pupil_right = max(pupil_right, na.rm=TRUE)) 

```


## age_cohort

```{r}
assert_that(all(data$age_cohort %in% c("adults","toddlers")))
```


## event_num

Why are there some event numbers that go to 26?

```{r}
unique(data$event_num)
```


```{r}
hist(data$event_num, breaks = 0:26)
```


Why are there so many instances of event 7?

```{r}
data |>
  group_by(lab_id, age_cohort, event_num) |>
  count()

```




# Saving 

Now save the merged xy data locally.

```{r saving}
save(data, file = here(INTERMEDIATE_FOLDER, INTERMEDIATE_002))
```

