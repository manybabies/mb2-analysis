---
title: "MB2-P - Multiverse Data Simulation and Statistical Analysis"
author: "Rmarkdown by Giulia Calignano, Marlena Mayer, Robert Hepach "
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    css: styles.css
  pdf_document: default
  word_document: default
  theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(MASS)
library(stringi)
set.seed(248)
```

## Data simulation

To facilitate the pre-registration of our data preprocessing and analysis strategies, the following R-routines detail a procedure for creating a simulated dataset that mirrors the characteristics of our anticipated data. This data simulation technique is instrumental in devising our data handling and analysis plans prior to actual data analysis The simulation includes generating random strings for identifiers and categorical variables, producing numeric data within defined ranges, managing missing values, and computing correlations. By examining the properties of the simulated dataset, we can ensure our analysis plan is robust and well-equipped to manage the complexities of the real dataset./

Of note, the dataset includes only the second trial and the two conditions x two outcome are presented in a long form with each of the *N* = 946 participants having time-series data for [one of] the four possible solutions./

Here the description of the dataset generated by the R-code:

- **participant_id**: A unique identifier for each participant, generated as a random string. Each participant has a unique ID to distinguish them in the dataset.
- **age_cohort**: A randomly generated string intended to categorize participants into different age cohorts.
- **t**: A numeric variable representing a timestamp or duration, generated within a specified range and with a possibility of missing values (NA).
- **x and y**: Numeric variables generated within specified ranges, intended to represent coordinates or other measurements, with a possibility of missing values.
- **pupil_left and pupil_right**: Numeric variables representing measurements of the left and right pupil sizes, respectively. These variables are generated with a specified correlation and then rescaled to a specific range.
- **lab_id**: A randomly generated string serving as a unique identifier for the lab or testing location.

- **conditions**: A categorical variable with two levels, i.e. "knowledge" and "ignorance"
- **outcomes**: A categorical variable with two levels, i.e. "incongruent" and "congruent"

```{r, SIMULATE DATA, echo=TRUE,include=TRUE, out.width='90%'}
# Install and load required packages
if (!require('dplyr')) install.packages('dplyr')
if (!require('tidyr')) install.packages('tidyr')
library(dplyr)
library(tidyr)

set.seed(123)  # For reproducibility
rm(list = ls())
# Number of participants
num_participants <- 946
num_toddlers <- 453
num_adults <- 493

# Generate participant IDs
participant_id <- as.character(1:num_participants)

# Generate age cohorts
age_cohort <- c(rep("toddlers", num_toddlers), rep("adults", num_adults))

# Generate time vector t
time_per_condition <- seq(0, 10000, by=25)
n_times <- length(time_per_condition)
n_conditions <- 4  # Two levels for condition and two levels for outcome

# Generate x and y coordinates
generate_coordinates <- function(n, mean_X, sd_X, min_X, max_X, mean_Y, sd_Y, min_Y, max_Y) {
  data.frame(
    x = pmin(pmax(rnorm(n, mean_X, sd_X), min_X), max_X),
    y = pmin(pmax(rnorm(n, mean_Y, sd_Y), min_Y), max_Y)
  )
}

# Generate pupil diameters with correlation
generate_pupils <- function(n, mean_left, sd_left, min_left, max_left, mean_right, sd_right, min_right, max_right, cor) {
  pupils <- MASS::mvrnorm(n, mu = c(mean_left, mean_right), 
                          Sigma = matrix(c(sd_left^2, cor * sd_left * sd_right, 
                                           cor * sd_left * sd_right, sd_right^2), 
                                         ncol = 2))
  pupils <- data.frame(
    pupil_left = pmin(pmax(pupils[,1], min_left), max_left),
    pupil_right = pmin(pmax(pupils[,2], min_right), max_right)
  )
  return(pupils)
}

# Generate conditions and outcomes
conditions <- c("knowledge", "ignorance")
outcomes <- c("incongruent", "congruent") # MSS: please check if this is correct! Does same refer to incongruent and empty to congruent as in choosing the same box and the empty box? But this would only be true for the ignorance condition but not for the knowledge condition, where the same box would be congruent and the empty box would be incongruent. I am somehow confused here. # RH: I think this is fine. The incongruent outcome in ignorance has the bear going to the empty box (and is thus comparable to the outcome in the knowledge condition). It's true that this is not 'incongruent' from the bear's perspective, 'empty box' would be a more descriptive term but I don't think it changes the dynamics of the simulation.

# Generate lab_id
lab_id <- rep(1:16, each = num_participants / 16)

# Helper function to generate random strings
generate_string <- function(n, length) {
  replicate(n, paste0(sample(LETTERS, length, replace = TRUE), collapse = ""))
}

# Assemble the dataset. The values are taken from the actual dataset.
simulate_data <- do.call(rbind, lapply(1:num_participants, function(pid) {
  cohort <- age_cohort[pid]
  if (cohort == "toddlers") {
    coord <- generate_coordinates(n_times * n_conditions, 794.8072342, 436.464115, -4433, 4534, 508.3721513, 305.1846593, -2474, 2430)
    pupils <- generate_pupils(n_times * n_conditions, 3.835746142, 0.693875755, 0.846, 7.997, 3.851782931, 0.68683815, 0.982, 6.824, 0.8)
  } else {
    coord <- generate_coordinates(n_times * n_conditions, 870.0972924, 406.371616, -5328.666667, 4625, 584.0366922, 342.2138564, -4222, 3588)
    pupils <- generate_pupils(n_times * n_conditions, 3.41242103, 0.628702372, 0.518, 7.567, 3.416267459, 0.632454602, 0.846, 6.46, 0.8)
  }
  data.frame(
    participant_id = rep(participant_id[pid], n_times * n_conditions),
    age_cohort = rep(cohort, n_times * n_conditions),
    t = rep(time_per_condition, n_conditions),
    condition = rep(conditions, each = n_times * 2),
    outcome = rep(rep(outcomes, each = n_times), 2),
    x = coord$x,
    y = coord$y,
    pupil_left = pupils$pupil_left,
    pupil_right = pupils$pupil_right,
    event_num = sample(1:9, n_times * n_conditions, replace = TRUE),
    lab_id = rep(lab_id[pid], n_times * n_conditions),
    distance = runif(n_times * n_conditions, 0, 0),
    target_side = generate_string(n_times * n_conditions, 2),
    bear_not_visible_ms = runif(n_times * n_conditions, 0, 31264),
    point_of_disambiguation = runif(n_times * n_conditions, 0, 35499),
    screen_width = sample(c(1280, 1920), n_times * n_conditions, replace = TRUE),
    screen_height = sample(c(720, 1080), n_times * n_conditions, replace = TRUE),
    point_zero = generate_string(n_times * n_conditions, 1),
    x_screen = runif(n_times * n_conditions, 0, 1930),
    y_screen = runif(n_times * n_conditions, 0, 1080),
    side = generate_string(n_times * n_conditions, 4),
    aoitype = generate_string(n_times * n_conditions, 5),
    aoi = generate_string(n_times * n_conditions, 15),
    Trial = rep(c(1,2,3,4), each = n_times))
}))

# Check how many data series there are per participant:

simulate_data |>
  filter(t<2) |>
  group_by(participant_id,condition, outcome, Trial) |>
  tally()

# Each participant is in the data 4 times but only one trials is needed. Pragmatic solution: Create a trial variable from 1 to 4 and then sample one trial per participant.

selected_trials <- simulate_data |>
  dplyr::group_by(participant_id) |>
  dplyr::slice_sample(n=1) |> 
  dplyr::select(participant_id, Trial) |>
  dplyr::ungroup()
  
simulate_data <- simulate_data %>%
  semi_join(selected_trials, by = c("participant_id", "Trial"))

```

## The Multiverse forking paths of pupillometry preprocessing 

The following script specifies the preprocessing and analysis steps of the multiverse approach applied to the MB2-P (see Calignano, Girardi, and Alto√©, 2023).

Let's assume that we have 10s worth of data per participant which starts at 1s before the resolution until 9s after the resultion. The crucial time-window for our analyses is the 5s after the bear exits the tubes, i.e,. from 1s to 6s in this dataset.

# First Degree of freedom: Filtering of extreme yet plausible pupil values, i.e.<2mm, >8mm  

```{r, DF1 EXTREME TONIC VALUES, echo=TRUE,include=TRUE, out.width='90%'}
#### plausible value range ]2,8[ - from 1 to 2 DATASETS
simulate_data$age_cohort = as.factor(simulate_data$age_cohort)
simulate_data$average <- (simulate_data$pupil_left + simulate_data$pupil_right)/2
all <-simulate_data
all$step <- "implausible"
summary(all)

plausible <-simulate_data[simulate_data$average>2 & simulate_data$average<8,] #RH: Does it matter that the number of rows is reduced as opposed to creating NAs and keeping number of rows the same?

plausible$step <- "plausible"
summary(plausible)

all_plausible <- list(all, plausible)

all$pupil_left = as.numeric(as.character(all$pupil_left))
all$pupil_right = as.numeric(as.character(all$pupil_right))
all = all[all$pupil_left >0 & all$pupil_right>0,]

all %>% mutate(Color = ifelse(pupil_left <=2 | pupil_left >=8 | pupil_right <=2 | pupil_right >=8,"blue", "red")) %>%
  ggplot(aes(x = pupil_left, y= pupil_right, color = Color))+
  geom_point(alpha= 0.2)+ xlab("pupil size left eye (mm)") + ylab("pupil size right eye (mm)") +
  scale_color_identity() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black")) + facet_wrap("age_cohort")

plot(all$x,all$y)
```

<!-- - Second degree of freedom: We consider three possible baseline corrections i.e., 5 seconds before the bear resolution vs 300 and 500 ms after the bear resolution. Correction is performed by subtracting the average pupil diameter from all subsequent values between, dividing those values by the average baseline, and averaging the baseline-corrected values vector. -->
# Second degree of freedom: We consider three possible baseline corrections i.e., 1s, 0.5s, or 0.25s before the bear resolution. Correction is performed by subtracting the average pupil diameter from all subsequent values between, dividing those values by the average baseline, and averaging the baseline-corrected values vector.

```{r, DF2 BASELINE-CORRECTION, echo=TRUE,include=TRUE, out.width='90%'}
### median baseline from 2 to 6 DATASETS # RH: Needed?

### 3 median baselines, i.e., initial time window for each trial_num ,id, lab_id
### i.e. 1s, 0.5s, or 0.25s before resolution.
library(plyr)
universe_one <- all_plausible
# 1s baseline 
    for (i in 1:2) { #RH: Looping through both plausible and all resulting from DF1.
         dg = 5 #RH: Not sure what this does?
           universe_one[[i]]<-ddply(universe_one[[i]],.(participant_id,condition, outcome, lab_id),
                  transform, baseline = average - mean(average[1:41], na.rm = T))
           }

#500 ms baseline 
universe_two <- all_plausible
  for (i in 1:2) {
    dg = 300 #RH: Not sure what this does?
    universe_two[[i]]<-ddply(universe_two[[i]],.(participant_id, condition, outcome, lab_id),
                  transform, baseline = average - mean(average[21:41], na.rm = T))
}

#250 ms baseline 
universe_three <-all_plausible
  for (i in 1:2) {
    dg = 500 #RH: Not sure what this does?
    universe_three[[i]]<-ddply(universe_three[[i]],.(participant_id, condition, outcome, lab_id),
                  transform, baseline = average - mean(average[31:41], na.rm = T))
}

#multiverse
multiverse <- list(universe_one, universe_two, universe_three)

#plot
library(gridExtra)
data_summary <- function(x) {
  m <- median(x)
  ymin <- m-sd(x)
  ymax <- m+sd(x)
  return(c(y=m,ymin=ymin,ymax=ymax))
}

ggplot(multiverse[[2]][[2]], aes(t,baseline,condition, colour =condition)) +
               labs(x = "time (ms)",y = "pupil size (mm)", colour = NULL)+
               geom_vline(xintercept = 0 )+ geom_smooth(se = T) + facet_wrap("outcome") + 
               geom_hline(yintercept = 0) +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                                                    panel.background = element_blank(), axis.line = element_line(colour = "black"))
                                               
ggplot(multiverse[[2]][[1]], aes(t,baseline,condition, colour =condition)) +
               labs(x = "time (ms)",y = "pupil size (mm)", colour = NULL)+
               geom_vline(xintercept = 0 )+ geom_smooth(se = T) + facet_wrap("outcome") + 
               geom_hline(yintercept = 0) +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                                                    panel.background = element_blank(), axis.line = element_line(colour = "black"))
  
```
At this point, the multiverse object contains a list of three elements, each with a nested structure consisting of two data frames. Each data frame has the same structure but differs in the observations they contain. Here is a breakdown of the structure:

List of 3: The top-level list contains three elements. Each element in this list is itself a list containing two data frames.

Each of the three lists:

Contains two data frames, each having 27 variables (columns).
The structure of these data frames is the same, but they have different numbers of observations (rows).
//
<br>

# Third degree of freedom: Participant exclusion (following the criteria of MB2) at the level of the 1st vs 2nd trial. 

Here we use two strings to exclude participants who provided valid data only on the second test trial (remove_ids_1) or who provided valid data on both test trials (remove_ids_2).

```{r, DF3 PARTICIPANT EXCLUSION, echo=TRUE,include=TRUE, out.width='90%'}
# Function to remove specific participant IDs from a multiverse
remove_participants <- function(multiverse, ids_to_remove) {
  lapply(multiverse, function(x) {
    lapply(x, function(df) {
      df[!df$participant_id %in% ids_to_remove, ]
    })
  })
}

# Define participant IDs to remove for the second and third multiverse
remove_ids_1 <- c(102, 103, 104, 105)
remove_ids_2 <- c(181, 182, 183, 185)

# Create the three multiverses
multiverse_one <- multiverse
multiverse_two <- remove_participants(multiverse, remove_ids_1)
multiverse_three <- remove_participants(multiverse, remove_ids_2)

# Combine into the megaverse
megaverse <- list(multiverse_one, multiverse_two, multiverse_three)

```
At this point we have an object called megaverse. The megaverse is a comprehensive collection of datasets organized into three different versions, each known as a multiverse. Each multiverse represents a different scenario or variation of the original dataset. Here's a detailed description of the structure:

Original Multiverse (multiverse_one): This version contains the unaltered original data. It serves as the baseline for comparison with other multiverses.
Modified Multiverse 1 (multiverse_two): In this version, certain participants are removed based on a specified list of participant IDs (remove_ids_1). This allows for analyzing how the removal of specific participants affects the results.
Modified Multiverse 2 (multiverse_three): This version excludes a different set of participants, defined by another list of participant IDs (remove_ids_2). This provides another perspective on the dataset with a different subset of participants removed.
Each multiverse consists of:

A list of lists, where each sublist contains one or more data frames.
Each data frame represents a segment of the dataset, including columns such as participant_id and other relevant variables (e.g., score).
By structuring the data in this way, the megaverse allows for systematic comparison across different scenarios, facilitating a robust analysis of the dataset under various conditions.

<br>

As a final step, we investigated (1) the averaged interaction effect Condition x Outcome, (2) the time-course of the interaction effect Condition x Outcome and (3) the non linear interaction effect of Condition x Outcome considering the time-course of the effect. This approach provided an exploration of whether and how smoothing time enhances the plausibility of statistical modeling of pupil dilation across the datasets we created.

For the analysis of pupillary data, we utilized generalized additive mixed modeling (GAMM, Wood, 2011). GAMMs combine the flexibility of generalized additive models (GAMs) with the ability to incorporate random effects, which are essential for accounting for correlations among observations within clusters or groups. This includes the use of smooth functions that handle both continuous and categorical predictors. The random effects component facilitates the inclusion of hierarchical structures, such as nested or repeated measures within the data, making GAMMs useful for a wide range of data types, including time series, spatial, and longitudinal data. The model is estimated using penalized regression techniques, which help to avoid overfitting and produce more reliable predictions.

```{r, SETTING MODEL, FUNCTIONS, echo=TRUE,include=TRUE, out.width='90%'}
library(mgcv)
library(itsadug)
library(sjPlot)
library(lme4)
## megaverse <- list(multiverse_one, multiverse_two, multiverse_three)

# Define a function to fit three different linear models to a dataset
fit_models <- function(data) {
  
  # Select data only from after the resolution, i.e., after 1s.
  
  data <- data |>
    filter(t>1000)
  
  # Model 1: Linear model with condition*outcome
  # This data needs to be averaged across the 5s window.
  model1.data <- simulate_data.trans |>
    filter(t > 1000 & t < 6000) |>
    group_by(participant_id, condition, outcome, age_cohort) |>
    dplyr::summarise(baseline.average = mean(baseline)) |>
    ungroup()
  model1 <- lm(baseline.average ~ condition*outcome*age_cohort, data = data)
  # model1 <- lmer(average ~ condition*outcome + (1|participant_id), data = data)
  
  # Model 2: Linear model with condition*outcome across the trial time
  model2 <-  lmer(baseline ~ t*condition*outcome*age_cohort + (1|participant_id), data = data)
  #RH: What about slopes for t on id?
  
  # Model 3: Linear model with both predictor variables 'x1' and 'x2'
  model3 <- bam(baseline~ condition*outcome + s(t, by= lab_id, k=7) + s(t, by= condition, k=20)+ s(t, by= outcome, k=20)+ s(t, participant_id, bs='fs', m=1), data= data, discrete=TRUE,nthreads=40)
  #RH: Need to fit age_cohort in here.
  
  # Store the models in a list
  models <- list(model1 = model1, model2 = model2, model3 = model3)
  
  return(models)
}

```

```{r, FITTING MODELS, echo=TRUE,include=TRUE, out.width='90%'}

#tba

```
