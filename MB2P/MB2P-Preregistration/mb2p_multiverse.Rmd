---
title: "MB2-P - Multiverse Data Simulation and Statistical Analysis"
author: "Rmarkdown by Giulia Calignano, Marlena Mayer, Robert Hepach "
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    css: styles.css
  pdf_document: default
  word_document: default
  theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(MASS)
library(stringi)
```

# Data simulation

To facilitate the pre-registration of our data preprocessing and analysis strategies, the following R-routines detail a procedure for creating a simulated dataset that mirrors the characteristics of our anticipated data. This data simulation technique is instrumental in devising our data handling and analysis plans prior to actual data analysis The simulation includes generating random strings for identifiers and categorical variables, producing numeric data within defined ranges, managing missing values, and computing correlations. By examining the properties of the simulated dataset, we can ensure our analysis plan is robust and well-equipped to manage the complexities of the real dataset.

Of note, the dataset includes only the second trial and the two conditions x two outcome are presented in a long form with each of the *N* = 946 participants having time-series data for [one of] the four possible solutions.

Here the description of the dataset generated by the R-code:

- **participant_id**: A unique identifier for each participant, generated as a random string. Each participant has a unique ID to distinguish them in the dataset.
- **age_cohort**: A randomly generated string intended to categorize participants into different age cohorts.
- **t**: A numeric variable representing a timestamp or duration, generated within a specified range and with a possibility of missing values (NA).
- **x and y**: Numeric variables generated within specified ranges, intended to represent coordinates or other measurements, with a possibility of missing values.
- **pupil_left and pupil_right**: Numeric variables representing measurements of the left and right pupil sizes, respectively. These variables are generated with a specified correlation and then rescaled to a specific range.
- **lab_id**: A randomly generated string serving as a unique identifier for the lab or testing location.
- **conditions**: A categorical variable with two levels, i.e. "knowledge" and "ignorance"
- **outcomes**: A categorical variable with two levels, i.e. "incongruent" and "congruent"

```{r, SIMULATE DATA, echo=TRUE,include=TRUE, out.width='90%'}
# Install and load required packages
if (!require('dplyr')) install.packages('dplyr')
if (!require('tidyr')) install.packages('tidyr')
library(dplyr)
library(tidyr)

set.seed(123)  # For reproducibility
rm(list = ls())
# Number of participants
num_participants <- 946
num_toddlers <- 453
num_adults <- 493

# Generate participant IDs
participant_id <- as.character(1:num_participants)

# Generate age cohorts
age_cohort <- c(rep("toddlers", num_toddlers), rep("adults", num_adults))

# Generate time vector t
time_per_condition <- seq(0, 10000, by=25)
n_times <- length(time_per_condition)
n_conditions <- 4  # Two levels for condition and two levels for outcome

# Generate x and y coordinates
generate_coordinates <- function(n, mean_X, sd_X, min_X, max_X, mean_Y, sd_Y, min_Y, max_Y) {
  data.frame(
    x = pmin(pmax(rnorm(n, mean_X, sd_X), min_X), max_X),
    y = pmin(pmax(rnorm(n, mean_Y, sd_Y), min_Y), max_Y)
  )
}

# Generate pupil diameters with correlation
generate_pupils <- function(n, mean_left, sd_left, min_left, max_left, mean_right, sd_right, min_right, max_right, cor) {
  pupils <- MASS::mvrnorm(n, mu = c(mean_left, mean_right), 
                          Sigma = matrix(c(sd_left^2, cor * sd_left * sd_right, 
                                           cor * sd_left * sd_right, sd_right^2), 
                                         ncol = 2))
  pupils <- data.frame(
    pupil_left = pmin(pmax(pupils[,1], min_left), max_left),
    pupil_right = pmin(pmax(pupils[,2], min_right), max_right)
  )
  return(pupils)
}

# Generate conditions and outcomes
conditions <- c("knowledge", "ignorance")
outcomes <- c("incongruent", "congruent") # MSS: please check if this is correct! Does same refer to incongruent and empty to congruent as in choosing the same box and the empty box? But this would only be true for the ignorance condition but not for the knowledge condition, where the same box would be congruent and the empty box would be incongruent. I am somehow confused here. # RH: I think this is fine. The incongruent outcome in ignorance has the bear going to the empty box (and is thus comparable to the outcome in the knowledge condition). It's true that this is not 'incongruent' from the bear's perspective, 'empty box' would be a more descriptive term but I don't think it changes the dynamics of the simulation.

# Generate lab_id
lab_id <- rep(1:16, each = num_participants / 16)

# Helper function to generate random strings
generate_string <- function(n, length) {
  replicate(n, paste0(sample(LETTERS, length, replace = TRUE), collapse = ""))
}

# Assemble the dataset. The values are taken from the actual dataset.
simulate_data <- do.call(rbind, lapply(1:num_participants, function(pid) {
  cohort <- age_cohort[pid]
  if (cohort == "toddlers") {
    coord <- generate_coordinates(n_times * n_conditions, 794.8072342, 436.464115, -4433, 4534, 508.3721513, 305.1846593, -2474, 2430)
    pupils <- generate_pupils(n_times * n_conditions, 3.835746142, 0.693875755, 0.846, 7.997, 3.851782931, 0.68683815, 0.982, 6.824, 0.8)
  } else {
    coord <- generate_coordinates(n_times * n_conditions, 870.0972924, 406.371616, -5328.666667, 4625, 584.0366922, 342.2138564, -4222, 3588)
    pupils <- generate_pupils(n_times * n_conditions, 3.41242103, 0.628702372, 0.518, 7.567, 3.416267459, 0.632454602, 0.846, 6.46, 0.8)
  }
  data.frame(
    participant_id = rep(participant_id[pid], n_times * n_conditions),
    age_cohort = rep(cohort, n_times * n_conditions),
    t = rep(time_per_condition, n_conditions),
    condition = rep(conditions, each = n_times * 2),
    outcome = rep(rep(outcomes, each = n_times), 2),
    x = coord$x,
    y = coord$y,
    pupil_left = pupils$pupil_left,
    pupil_right = pupils$pupil_right,
    event_num = sample(1:9, n_times * n_conditions, replace = TRUE),
    lab_id = rep(lab_id[pid], n_times * n_conditions),
    distance = runif(n_times * n_conditions, 0, 0),
    target_side = generate_string(n_times * n_conditions, 2),
    bear_not_visible_ms = runif(n_times * n_conditions, 0, 31264),
    point_of_disambiguation = runif(n_times * n_conditions, 0, 35499),
    screen_width = sample(c(1280, 1920), n_times * n_conditions, replace = TRUE),
    screen_height = sample(c(720, 1080), n_times * n_conditions, replace = TRUE),
    point_zero = generate_string(n_times * n_conditions, 1),
    x_screen = runif(n_times * n_conditions, 0, 1930),
    y_screen = runif(n_times * n_conditions, 0, 1080),
    side = generate_string(n_times * n_conditions, 4),
    aoitype = generate_string(n_times * n_conditions, 5),
    aoi = generate_string(n_times * n_conditions, 15),
    Trial = rep(c(1,2,3,4), each = n_times))
}))

# Introduce NA values randomly in the dataset
na_percentage <- 0.5  # Percentage of NA values
columns_to_na <- c("x", "y", "pupil_left", "pupil_right")

simulate_data <- simulate_data %>%
  mutate(across(all_of(columns_to_na), ~ ifelse(runif(n()) < na_percentage, NA, .)))

# Ensure at least 50% non-NA in Trial
valid_trials <- simulate_data %>%
  filter(!is.na(Trial)) %>%
  dplyr::count(participant_id)
participants_to_keep <- valid_trials %>%
  filter(n > 2)  # Ensure more than 2 valid trials per participant (i.e., > 50%)
simulate_data <- simulate_data %>%
  filter(participant_id %in% participants_to_keep$participant_id)


# Check how many data series there are per participant:

simulate_data |>
  filter(t<2) |>
  group_by(participant_id,condition, outcome, Trial) |>
  tally()

# Each participant is in the data 4 times but only one trials is needed. Pragmatic solution: Create a trial variable from 1 to 4 and then sample one trial per participant.

selected_trials <- simulate_data |>
  dplyr::group_by(participant_id) |>
  dplyr::slice_sample(n=1) |> 
  dplyr::select(participant_id, Trial) |>
  dplyr::ungroup()

simulate_data <- simulate_data %>%
  semi_join(selected_trials, by = c("participant_id", "Trial"))

```

## The Multiverse forking paths of pupillometry preprocessing 

The following script specifies the preprocessing and analysis steps of the multiverse approach applied to the MB2-P (see Calignano, Girardi, and Alto√©, 2023).

Let's assume that we have 10s worth of data per participant which starts at 1s before the resolution until 9s after the resultion. The crucial time-window for our analyses is the 5s after the bear exits the tubes, i.e,. from 1s to 6s in this dataset.

# First Degree of freedom: Filtering of extreme yet plausible pupil values, i.e.<2mm, >8mm  

```{r, DF1 EXTREME TONIC VALUES, echo=TRUE,include=TRUE, out.width='90%'}
#### plausible value range ]2,8[ - from 1 to 2 DATASETS
simulate_data$age_cohort = as.factor(simulate_data$age_cohort)
simulate_data$average <- (simulate_data$pupil_left + simulate_data$pupil_right)/2
all <-simulate_data
all$step <- "implausible"
summary(all)

plausible <-simulate_data[simulate_data$average>2 & simulate_data$average<8,] #RH: Does it matter that the number of rows is reduced as opposed to creating NAs and keeping number of rows the same? #GC in terms of computational problems there are no differences 

plausible$step <- "plausible"
summary(plausible)

all_plausible <- list(all, plausible)

all$pupil_left = as.numeric(as.character(all$pupil_left))
all$pupil_right = as.numeric(as.character(all$pupil_right))
all = all[all$pupil_left >0 & all$pupil_right>0,]

all %>% mutate(Color = ifelse(pupil_left <=2 | pupil_left >=8 | pupil_right <=2 | pupil_right >=8,"blue", "red")) %>%
  ggplot(aes(x = pupil_left, y= pupil_right, color = Color))+
  geom_point(alpha= 0.2)+ xlab("pupil size left eye (mm)") + ylab("pupil size right eye (mm)") +
  scale_color_identity() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black")) + facet_wrap("age_cohort")

plot(all$x,all$y)
```

# Second degree of freedom: within the screen vs outside the screen

```{r, DF2 Area Of Interest, echo=TRUE,include=TRUE, out.width='90%'}
AOI_all <- all[all$x > 0,]
AOI_all <- AOI_all[AOI_all$x < 2000,]
AOI_all <- AOI_all[AOI_all$y > 0 ,]
AOI_all <- AOI_all[AOI_all$y <1000,]
AOI_all$step <- "AOI_implausible"
summary(AOI_all)

AOI_plausible <-plausible[plausible$x > 0,]
AOI_plausible <- AOI_plausible[AOI_plausible$x < 2000,]
AOI_plausible <- AOI_plausible[AOI_plausible$y > 0 ,]
AOI_plausible <- AOI_plausible[AOI_plausible$y <1000,]
AOI_plausible$step <- "AOI_plausible"
summary(AOI_plausible)

AOI <- list(AOI_all, AOI_plausible, all, plausible)
plot(AOI_all$x,AOI_all$y)

```

# Third degree of freedom: moving average filtered vs unfiltered data

```{r, DF3 moving average, echo=TRUE,include=TRUE, out.width='90%'}
# Install and load required packages
if (!require('dplyr')) install.packages('dplyr')
if (!require('zoo')) install.packages('zoo')
library(dplyr)
library(zoo)
#empty object
AOI_MA <- AOI
# Function to apply moving average to a single dataset
apply_moving_average <- function(data, window_size = 5) {
  if (!is.data.frame(data)) {
    stop("The provided data is not a data frame")
  }
  data %>%
    group_by(participant_id, Trial) %>%
    mutate(
      pupil_left_MA = zoo::rollapply(pupil_left, width = window_size, FUN = mean, fill = NA, align = 'right'),
      pupil_right_MA = zoo::rollapply(pupil_right, width = window_size, FUN = mean, fill = NA, align = 'right')
    ) %>%
    ungroup()
}

# Iterate through the AOI list of lists and apply the moving average function
for (i in seq_along(AOI_MA)) {
  for (j in seq_along(AOI_MA[[i]])) {
    if (is.data.frame(AOI_MA[[i]][[j]])) {
      AOI_MA[[i]][[j]] <- apply_moving_average(AOI[[i]][[j]])
    } 
  }
}

MA <- list(AOI, AOI_MA)

```

# Fourth degree of freedom: 1s, 0.5s, or 0.25s before the bear resolution.

We consider three possible baseline correction all performed by subtracting the average pupil diameter from all subsequent values between, dividing those values by the average baseline, and averaging the baseline-corrected values vector.

```{r, DF4 BASELINE-CORRECTION, echo=TRUE,include=TRUE, out.width='90%'}
### 3 median baselines, i.e., initial time window for each trial_num ,id, lab_id
### i.e. 1s, 0.5s, or 0.25s before resolution.
library(plyr)
universe_one <- MA
# 1s baseline 

           # Iterate through each list within universe_one
for (i in seq_along(universe_one)) {
  # Iterate through each data frame within the current list
  for (j in seq_along(universe_one[[i]])) {
    # Check if the current element is a data frame
    if (is.data.frame(universe_one[[i]][[j]])) {
      # Apply the baseline correction
      universe_one[[i]][[j]] <- ddply(universe_one[[i]][[j]], .(participant_id, condition, outcome, lab_id), transform, baseline = {        avg_filtered <- average[1:41]
                                        avg_mean <- mean(avg_filtered, na.rm = TRUE)
                                        average - avg_mean
                                      })
    } else {
      warning(paste("Element universe_one[[", i, "]][[", j, "]] is not a data frame and will be skipped.", sep = ""))
    }
  }
}


#500 ms baseline 
universe_two <- MA
for (i in seq_along(universe_two)) {
  # Iterate through each data frame within the current list
  for (j in seq_along(universe_two[[i]])) {
    # Check if the current element is a data frame
    if (is.data.frame(universe_two[[i]][[j]])) {
      # Apply the baseline correction
      universe_two[[i]][[j]] <- ddply(universe_two[[i]][[j]], .(participant_id, condition, outcome, lab_id), transform, baseline = {        avg_filtered <- average[21:41]
                                        avg_mean <- mean(avg_filtered, na.rm = TRUE)
                                        average - avg_mean
                                      })
    } else {
      warning(paste("Element universe_one[[", i, "]][[", j, "]] is not a data frame and will be skipped.", sep = ""))
    }
  }
}

#250 ms baseline 
universe_three <- MA
 for (i in seq_along(universe_three)) {
  # Iterate through each data frame within the current list
  for (j in seq_along(universe_three[[i]])) {
    # Check if the current element is a data frame
    if (is.data.frame(universe_three[[i]][[j]])) {
      # Apply the baseline correction
      universe_three[[i]][[j]] <- ddply(universe_three[[i]][[j]], .(participant_id, condition, outcome, lab_id), transform, baseline = {        avg_filtered <- average[31:41]
                                        avg_mean <- mean(avg_filtered, na.rm = TRUE)
                                        average - avg_mean
                                      })
    } else {
      warning(paste("Element universe_three[[", i, "]][[", j, "]] is not a data frame and will be skipped.", sep = ""))
    }
  }
}

#multiverse
multiverse <- list(universe_one, universe_two, universe_three)

#plot
library(gridExtra)
data_summary <- function(x) {
  m <- median(x)
  ymin <- m-sd(x)
  ymax <- m+sd(x)
  return(c(y=m,ymin=ymin,ymax=ymax))
}

ggplot(multiverse[[2]][[2]][[2]], aes(t,baseline,condition, colour =condition)) +
               labs(x = "time (ms)",y = "pupil size (mm)", colour = NULL)+
               geom_vline(xintercept = 0 )+ geom_smooth(se = T) + facet_wrap("outcome") + 
               geom_hline(yintercept = 0) +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                                                    panel.background = element_blank(), axis.line = element_line(colour = "black"))
                                               
ggplot(multiverse[[2]][[1]][[1]], aes(t,baseline,condition, colour =condition)) +
               labs(x = "time (ms)",y = "pupil size (mm)", colour = NULL)+
               geom_vline(xintercept = 0 )+ geom_smooth(se = T) + facet_wrap("outcome") + 
               geom_hline(yintercept = 0) +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                                                    panel.background = element_blank(), axis.line = element_line(colour = "black"))
  
```
At this point, the multiverse object contains a list of three elements, each with a nested structure consisting of four data frames. Each data frame has the same structure but differs in the observations they contain. Here is a breakdown of the structure:

List of 3: The top-level list contains three elements. Each element in this list is itself a list containing four data frames.

Each of the three lists:

Contains four data frames, each having 27 variables (columns).
The structure of these data frames is the same, but they have different numbers of observations (rows).
//
<br>

# Fifth degree of freedom: Participant exclusion (following the criteria of MB2) at the level of the 1st vs 2nd trial. 

Here we use two strings to exclude participants who provided valid data only on the second test trial (remove_ids_1) or who provided valid data on both test trials (remove_ids_2).

```{r, DF5 PARTICIPANT EXCLUSION, echo=TRUE,include=TRUE, out.width='90%'}
# Load necessary library
library(dplyr)

# Function to handle nested lists and remove specific participant_ids
remove_participants <- function(multiverse, participant_ids_to_remove) {

  clean_data <- function(data) {
    # Check if the element is a data frame
    if (is.data.frame(data)) {
      return(data %>% filter(!participant_id %in% participant_ids_to_remove))
    }
    # If it's a list, apply the function recursively
    else if (is.list(data)) {
      return(lapply(data, clean_data))
    }
    # If it's neither, return it unchanged (or NULL if you prefer skipping non-list, non-data.frame items)
    else {
      return(NULL)
    }
  }

  # Apply the cleaning function at the top level
  cleaned_multiverse <- lapply(multiverse, clean_data)

  return(cleaned_multiverse)
}

# Define participant IDs to remove for the second and third multiverse
remove_ids_1 <- c(101, 102, 103)
remove_ids_2 <- c(181, 182, 183, 185)

# Create the three multiverses
multiverse_one <- multiverse
multiverse_two <- remove_participants(multiverse, remove_ids_1)
multiverse_three <- remove_participants(multiverse, remove_ids_2)

# Combine into the megaverse
megaverse <- list(multiverse_one, multiverse_two, multiverse_three)

```
At this point we have an object called megaverse. The megaverse is a comprehensive collection of datasets organized into three different versions, each known as a multiverse. Each multiverse represents a different scenario or variation of the original dataset. Here's a detailed description of the structure:

Original Multiverse (multiverse_one): This version contains the unaltered original data. It serves as the baseline for comparison with other multiverses.
Modified Multiverse 1 (multiverse_two): In this version, certain participants are removed based on a specified list of participant IDs (remove_ids_1). This allows for analyzing how the removal of specific participants affects the results.
Modified Multiverse 2 (multiverse_three): This version excludes a different set of participants, defined by another list of participant IDs (remove_ids_2). This provides another perspective on the dataset with a different subset of participants removed.
Each multiverse consists of:

A list of lists, where each sublist contains one or more data frames.
Each data frame represents a segment of the dataset, including columns such as participant_id and other relevant variables included in the simulated data.
By structuring the data in this way, the megaverse allows for systematic comparison across different scenarios, facilitating a robust analysis of the dataset under various conditions.

<br>

As a final step, we investigated (1) the averaged interaction effect Condition x Outcome, (2) the time-course of the interaction effect Condition x Outcome and (3) the non linear interaction effect of Condition x Outcome considering the time-course of the effect. This approach provided an exploration of whether and how smoothing time enhances the plausibility of statistical modeling of pupil dilation across the datasets we created.

For the analysis of pupillary data, we utilized generalized additive mixed modeling (GAMM, Wood, 2011). GAMMs combine the flexibility of generalized additive models (GAMs) with the ability to incorporate random effects, which are essential for accounting for correlations among observations within clusters or groups. This includes the use of smooth functions that handle both continuous and categorical predictors. The random effects component facilitates the inclusion of hierarchical structures, such as nested or repeated measures within the data, making GAMMs useful for a wide range of data types, including time series, spatial, and longitudinal data. The model is estimated using penalized regression techniques, which help to avoid overfitting and produce more reliable predictions.

```{r, SETTING MODEL, FUNCTIONS, echo=TRUE,include=TRUE, out.width='90%'}
# Inspect the structure of megaverse
str(megaverse, max.level = 3)

extract_data_frame <- function(data) {
  while (is.list(data) && length(data) == 1) {
    data <- data[[1]]
  }
  return(data)
}

library(dplyr)
library(lme4)
library(mgcv)
library(purrr)

# Function to check if all dataset are a data frame
is_valid_dataset <- function(data) {
  return(is.data.frame(data))
}

# Pre-validation: Validate all datasets in megaverse with debug information
validate_megaverse <- function(megaverse) {
  validation_results <- map2(megaverse, seq_along(megaverse), function(sublist, i) {
    if (is.list(sublist)) {
      return(map2(sublist, seq_along(sublist), function(innerlist, j) {
        if (is.list(innerlist)) {
          return(map2(innerlist, seq_along(innerlist), function(innermostlist, k) {
            if (is.list(innermostlist)) {
              return(map2(innermostlist, seq_along(innermostlist), function(dataset, l) {
                dataset <- extract_data_frame(dataset)
                cat(sprintf("Validating dataset at [%d][%d][%d][%d]: Type = %s\n", i, j, k, l, class(dataset)))
                if (!is_valid_dataset(dataset)) {
                  warning(sprintf("Dataset at [%d][%d][%d][%d] is not a valid data frame. Type: %s", i, j, k, l, class(dataset)))
                  return(NULL)  # Mark invalid datasets as NULL
                }
                cat(sprintf("Structure of dataset at [%d][%d][%d][%d]:\n", i, j, k, l))
                print(str(dataset))
                return(TRUE)  # Mark valid datasets
              }))
            } else {
              warning(sprintf("Expected list of lists at [%d][%d][%d], but found: %s", i, j, k, class(innermostlist)))
              return(NULL)
            }
          }))
        } else {
          warning(sprintf("Expected list of lists at [%d][%d], but found: %s", i, j, class(innerlist)))
          return(NULL)
        }
      }))
    } else {
      warning(sprintf("Expected list of lists at index %d, but found: %s", i, class(sublist)))
      return(NULL)
    }
  })
  return(validation_results)
}

# Define the function to fit the models to a dataset
fit_models <- function(data) {
  models <- list()
  if (is_valid_dataset(data)) {
    # Model 1: Linear model with condition*outcome
    model1_data <- data %>%
      filter(t > 1000 & t < 6000) %>%
      group_by(participant_id, condition, outcome, age_cohort) %>%
      dplyr::summarise(baseline_average = mean(baseline, na.rm = TRUE)) %>%
      ungroup()
    models$model1 <- tryCatch(lm(baseline_average ~ condition * outcome * age_cohort, data = model1_data), error = function(e) e)

    # Model 2: Linear mixed model with condition*outcome across trial time
    models$model2 <- tryCatch(lmer(baseline ~ t * condition * outcome * age_cohort + (1 + t | participant_id), data = data), error = function(e) e)

    # Model 3: Generalized additive model with smooth terms
    models$model3 <- tryCatch(
      bam(baseline ~ condition * outcome * age_cohort + 
      s(t, by = lab_id, k=7) +
      s(t, by = condition, k=20) +
      s(t, by = outcome, k=20) + 
      s(t, participant_id, bs = 'fs', m = 1), 
      data = data, discrete = TRUE, nthreads = 40), 
      error = function(e) e
    )
  } else {
    models$error <- "Provided data is not a valid data frame."
  }
  return(models)
}

# Model fitting: Process valid datasets in megaverse
fit_models_to_valid_datasets <- function(megaverse, validation_results) {
  model_results <- map2(megaverse, validation_results, function(sublist, valid_sublist) {
    if (is.list(sublist) && !is.null(valid_sublist)) {
      return(map2(sublist, valid_sublist, function(innerlist, valid_innerlist) {
        if (is.list(innerlist) && !is.null(valid_innerlist)) {
          return(map2(innerlist, valid_innerlist, function(innermostlist, valid_innermostlist) {
            if (is.list(innermostlist) && !is.null(valid_innermostlist)) {
              return(map2(innermostlist, valid_innermostlist, function(dataset, is_valid) {
                dataset <- extract_data_frame(dataset)
                if (is.null(is_valid)) {
                  return(list(error = "Invalid data frame. Skipped processing."))
                }
                return(fit_models(dataset))
              }))
            }
            return(NULL)
          }))
        }
        return(NULL)
      }))
    }
    return(NULL)
  })
  return(model_results)
}

# Validate all datasets first
validation_results <- validate_megaverse(megaverse)

# Fit models to only valid datasets
megaverse_model_results <- fit_models_to_valid_datasets(megaverse, validation_results)

# Print the results (or handle them as needed)
cat("Model fitting results:\n")
print(megaverse_model_results)


```
To select the best model based on the lowest BIC, lowest AIC, and highest R-squared (Calignano et al., 2024), we computed the aboved cited statistics for each model and then compare them. We create a function to extract these metrics, identify the best model based on each criterion, and then plot the effects of the best model.

```{r, MODEL SELECTION, echo=TRUE,include=TRUE, out.width='90%'}
# Function to extract BIC, AIC, and R-squared from a model
extract_model_metrics <- function(model) {
  if (inherits(model, "lm") || inherits(model, "lmerMod") || inherits(model, "bam")) {
    aic_value <- AIC(model)
    bic_value <- BIC(model)
    r_squared <- if (inherits(model, "lm")) {
      summary(model)$r.squared
    } else if (inherits(model, "lmerMod")) {
      r.squaredGLMM(model)[1]  # Marginal R-squared for mixed models
    } else if (inherits(model, "bam")) {
      summary(model)$r.sq
    } else {
      NA
    }
    return(list(AIC = aic_value, BIC = bic_value, R_squared = r_squared))
  } else {
    return(NULL)
  }
}

# Function to compare models and select the best based on AIC, BIC, and R-squared
compare_models <- function(models_list) {
  model_metrics <- map(models_list, ~map(.x, extract_model_metrics))
  
  best_models <- list()
  best_models$AIC <- model_metrics %>%
    map_df(~map_df(.x, ~data.frame(AIC = .x$AIC)), .id = "model") %>%
    arrange(AIC) %>%
    slice(1)
  
  best_models$BIC <- model_metrics %>%
    map_df(~map_df(.x, ~data.frame(BIC = .x$BIC)), .id = "model") %>%
    arrange(BIC) %>%
    slice(1)
  
  best_models$R_squared <- model_metrics %>%
    map_df(~map_df(.x, ~data.frame(R_squared = .x$R_squared)), .id = "model") %>%
    arrange(desc(R_squared)) %>%
    slice(1)
  
  return(best_models)
}

library(ggplot2)

# Function to plot the effects of a model
plot_model_effects <- function(model) {
  if (inherits(model, "lm")) {
    plot_data <- data.frame(
      Fitted = fitted(model),
      Residuals = residuals(model)
    )
    ggplot(plot_data, aes(Fitted, Residuals)) +
      geom_point() +
      geom_smooth(method = "loess") +
      labs(title = "Effects of the Best Linear Model")
  } else if (inherits(model, "lmerMod")) {
    plot_data <- data.frame(
      Fitted = fitted(model),
      Residuals = residuals(model)
    )
    ggplot(plot_data, aes(Fitted, Residuals)) +
      geom_point() +
      geom_smooth(method = "loess") +
      labs(title = "Effects of the Best Linear Mixed Model")
  } else if (inherits(model, "bam")) {
    plot(model, pages = 1, main = "Effects of the Best Generalized Additive Model")
  } else {
    stop("Unknown model type")
  }
}


```
