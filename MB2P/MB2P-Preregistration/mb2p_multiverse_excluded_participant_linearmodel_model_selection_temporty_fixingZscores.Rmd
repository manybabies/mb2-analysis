---
title: "MB2-P - Multiverse Data Simulation and Statistical Analysis"
author: "Rmarkdown by Giulia Calignano, Melanie S. Schreiner, Alvin Tan, and Robert Hepach"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    css: styles.css
  pdf_document: default
  word_document: default
  theme: united
---

```{r setup, include=FALSE}
# Clear workspace:
rm(list = ls())

knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(zoo)
library(mgcv)
library(here)
library(dplyr)
library(tidyverse)
library(stringi)
library(gridExtra)
library(purrr)
library(broom)
library(broom.mixed)
library(lme4)
library(performance)
```

```{r load data, include=FALSE}
set.seed(123) # for reproducibility
source(here("helper", "ensure_repo_structure.R"))
theme_set(theme_classic())

# load pupillometry data
load(here(INTERMEDIATE_FOLDER, INTERMEDIATE_008))
rm(list = setdiff(ls(), c("data_pupillometry", "INTERMEDIATE_FOLDER", "INTERMEDIATE_008", "RESULTS_FOLDER")))
```

## The Data 

- **participant_id**: A unique identifier for each participant, generated as a random string. Each participant has a unique ID to distinguish them in the dataset.
- **age_cohort**: A randomly generated string intended to categorize participants into different age cohorts.
- **t**: A numeric variable representing a timestamp or duration, generated within a specified range and with a possibility of missing values (NA).
- **x and y**: Numeric variables generated within specified ranges, intended to represent coordinates or other measurements, with a possibility of missing values.
- **pupil_left and pupil_right**: Numeric variables representing measurements of the left and right pupil sizes, respectively. These variables are generated with a specified correlation and then rescaled to a specific range.
- **lab_id**: A randomly generated string serving as a unique identifier for the lab or testing location.
- **conditions**: A categorical variable with two levels, i.e. "knowledge" and "ignorance"
- **outcomes**: A categorical variable with two levels, i.e. "incongruent" and "congruent"

## The Multiverse forking paths of pupillometry preprocessing 

The following script specifies the preprocessing and analysis steps of the multiverse approach applied to the MB2-P (see Calignano, Girardi, and Alto√©, 2023).

```{r preprocess}
#! The z-transformation does not work. It produces weird values in the plots below. Next step: We have to transform the values here, per subject per recording, to preserve the shape of the signal.

data_pupillometry.temp <- data_pupillometry
data_pupillometry <- data_pupillometry.temp

data_pupillometry <- data_pupillometry |> 
  filter(trial_num == 6) |>                   # filter to second test trial only (data reduction)
  select(participant_id, participant_lab_id, lab_id, age_cohort, t, x, y, pupil_left, pupil_right, condition, t_norm, trial_num, participant_gender, eyetracker_type, outcome, pilot, session_error, age_exclusion, sufficient_fam_trials) |>
  mutate(pupil_left = ifelse(pupil_left < 0 | pupil_right < 0, NA, pupil_left),
         pupil_right = ifelse(pupil_left < 0 | pupil_right < 0, NA, pupil_right)
         ) |>
  mutate(age_cohort <- as.factor(age_cohort)) |> 
  rowwise() |> 
  mutate(average = mean(c(pupil_left, pupil_right), na.rm = TRUE)) |> # switched to liberal avg
  ungroup() |>
  group_by(participant_id) |> # Scale pupil-value #! Should we be using participant_lab_id here?
  mutate(average_z = (average - mean(average, na.rm = TRUE)) / sd(average, na.rm = TRUE)) |>
  ungroup() |>  
  filter(t_norm >= -1000 & t_norm < 5000)  

hist(data_pupillometry$average)
hist(data_pupillometry$average_z) # Looks evenly distributed.

# centering of age_cohort, condition, and outcome
  data_pupillometry <- data_pupillometry |>
    mutate(
    age_cohort_c = case_when(
      age_cohort == "adults" ~ -0.5,
      age_cohort == "toddlers" ~ 0.5
    ),
    condition_c = case_when(
      condition == "knowledge" ~ -0.5,
      condition == "ignorance" ~ 0.5
    ),
    outcome_c = case_when(
      outcome == "congruent" ~ -0.5,
      outcome == "incongruent" ~ 0.5)
    )
  
# z-transform left and right pupil size within each eye tracker type
# data_pupillometry <- data_pupillometry |>
#   group_by(participant_id) |> # Should this be done by age cohort, too?
#   mutate(
#     pupil_left_z = (pupil_left - mean(pupil_left, na.rm = TRUE)) / sd(pupil_left, na.rm = TRUE),
#     pupil_right_z = (pupil_right - mean(pupil_right, na.rm = TRUE)) / sd(pupil_right, na.rm = TRUE)) |>
#   ungroup() |>
#   rowwise() |>
#   mutate(average_z = mean(c(pupil_left_z, pupil_right_z), na.rm = TRUE)) |>
#   ungroup()
  
# Merging in AL data (variable prop_exit)

summarize_participant_test_both_trials <- readRDS(here(RESULTS_FOLDER,"summarize_participant_test_both_trials.rds"))

data_pupillometry <- data_pupillometry %>% left_join(summarize_participant_test_both_trials %>% dplyr::select("participant_lab_id", "trial_num", "prop_exit"), by = c( "participant_lab_id", "trial_num"))

rm(summarize_participant_test_both_trials)

# # For the subsequent step, i.e., first degree, identify those z-values that correspond to 2mm and 8mm (according to Tobii).
temp <- data_pupillometry |> 
  filter(eyetracker_type != "EyeLink") |>
  group_by(participant_id) |>
  filter(average <= 2) |>
  select(participant_id, average, average_z) |>
  arrange(average_z) |>
  ungroup()
#
temp |>
  summarise(ZMean.2mm = mean(average_z, na.rm = T), ZMedian.2mm = median(average_z, na.rm = T), Min.2mm = min(average, na.rm = T), Max.2mm = max(average, na.rm = T),
            ZMin.2mm = min(average_z, na.rm = T), ZMax.2mm = max(average_z, na.rm = T))

## -> Lower bound: z = 0.8000874
hist(temp$average_z)
rm("temp")
##
temp <- data_pupillometry |> 
  filter(eyetracker_type != "EyeLink") |>
  group_by(participant_id) |>
  filter(average >= 8) |>
  select(participant_id, average, average_z) |>
  arrange(average_z) |>
  ungroup()
#
temp |>
  summarise(Min.8mm = min(average, na.rm = T), Max.8mm = max(average, na.rm = T),
            ZMin.8mm = min(average_z, na.rm = T), ZMax.8mm = max(average_z, na.rm = T))

## -> There are no samples with pupil_size ?> 8mm
```
# First Degree of freedom: Filtering of extreme yet plausible pupil values, i.e.<2mm, >8mm  
This results in 2 datasets; (plausible vs. implausible values)

```{r DF1 extreme tonic values, echo=TRUE, include=TRUE, out.width='90%'}
# data_plausible <- data_pupillometry |> 
#     mutate(average = ifelse(average_z < 0.8000874 , NA, average)) |>
#     mutate(average_z = ifelse(average_z < 0.8000874, NA, average_z))

data_plausible <- data_pupillometry |> 
  mutate(average = ifelse(eyetracker_type != "EyeLink" & average <= 2 , NA, average),
         average_z = ifelse(eyetracker_type != "EyeLink" & average <= 2 , NA, average_z)
         )

data_df1 <- bind_rows(
  data_pupillometry |> 
    mutate(df1_extreme_values = "implausible"),
  data_plausible |> 
    mutate(df1_extreme_values = "plausible")
) |> nest(data = -df1_extreme_values) 

# scatterplot
data_pupillometry |> 
  filter(eyetracker_type != "EyeLink") |>
  mutate(plausibility = ifelse(pupil_left <= 2 | pupil_left >= 8 | 
                                 pupil_right <= 2 | pupil_right >= 8,
                               "implausible", "plausible")) |> 
  ggplot(aes(x = pupil_left, y = pupil_right, col = plausibility)) +
  geom_point(alpha = .1) +
  labs(x = "Pupil size left eye (mm)",
       y = "Pupil size right eye (mm)") +
  scale_color_manual(values = c("indianred", "royalblue")) +
  facet_wrap("age_cohort")

# Check to see  whether any changes were made.
data_df1 |> 
  pull(data) |> 
  map(\(d) {
  d |>
  group_by(eyetracker_type) |>
  summarise(AVE = mean(average_z, na.rm =T))
  })  
```

# Second degree of freedom: gaze within the screen vs outside the screen
This results in 4 datasets; 2 (plausible vs. implausible values) * 2 (on-screen vs. off-screen).

```{r DF2 area of interest, echo=TRUE, include=TRUE, out.width='90%'}
#! Does the following actually affect the cols average or average_z?
data_within <- data_df1 |> 
  mutate(data = map(data, \(d) {
    d |> 
      mutate(x = ifelse(x <= 0 | x >= 1280, NA, x),
             y = ifelse(y <= 0 | y >= 960, NA, y),
            average = ifelse(is.na(x) | is.na(y), NA, average),
            average_z = ifelse(is.na(x) | is.na(y), NA, average_z)
            )
  }))

data_df2 <- bind_rows(
  data_df1 |> mutate(df2_screen_fixation = "outside", .before = "data"),
  data_within |> mutate(df2_screen_fixation = "within", .before = "data")
)

#! Can we adjust this to show both 'mapped 'in' and 'out' values?
#! Can we adjust this to show both 'mapped 'in' and 'out' values?
data_pupillometry |> 
  mutate(screen_fixation = ifelse(x <= 0 | x >= 1280 | y <= 0 | y >= 960,
                               "outside", "within")) |>
  ggplot(aes(x = x, y = y, col = screen_fixation)) +
    scale_color_manual(name = "screen fixation", values = c("indianred", "royalblue")) +
    facet_wrap("age_cohort") +
    geom_point(alpha = .1)

data_df2 |> 
  pull(data) |> 
  map(\(d) {
  d |>
  group_by(eyetracker_type) |>
  summarise(AVE = mean(average_z, na.rm =T))
  })  
```

# Third degree of freedom: moving average filtered vs unfiltered data
This results in 8 datasets; 2 (plausible vs. implausible values) * 2 (on-screen vs. off-screen) * 2 (filtered or not).

```{r DF3 moving average, echo=TRUE, include=TRUE, out.width='90%'}
WINDOW_SIZE = 5

data_avg <- data_df2 |> 
  mutate(data = map(data, \(d) { 
    d |> 
      group_by(participant_id, trial_num) |> 
      mutate(average = rollapply(average, width = WINDOW_SIZE, 
                                     FUN = mean, na.rm = TRUE, 
                                     fill = NA, align = "right"),
              average_z = rollapply(average_z, width = WINDOW_SIZE, 
                                      FUN = mean, na.rm = TRUE, 
                                      fill = NA, align = "right")) |>
      ungroup() 
  }))

data_df3 <- bind_rows(
  data_df2 |> mutate(df3_moving_average = "unfiltered", .before = "data"),
  data_avg |> mutate(df3_moving_average = "filtered", .before = "data")
)

data_df3 |> 
  pull(data) |> 
  (`[`)(value = c(1:4)) |>
  map(\(d) {
    ggplot(d |> filter(!is.na(average_z),
                       t_norm >= -1000 & t_norm < 5000), 
           aes(x = t_norm, y = average_z, col = outcome, linetype = age_cohort)) +
      labs(x = "Time (ms)",
           y = "Pupil size (z-score)") +
      geom_smooth(se = TRUE) +
      facet_grid(condition ~ age_cohort)
  })

#! Why is average pupil size for adults *smaller* than for children?!

data_df3 |> 
  pull(data) |> 
  map(\(d) {
  d |>
  group_by(eyetracker_type) |>
  summarise(AVE = mean(average_z, na.rm =T))
  })  

```

# Fourth degree of freedom: 1s, 0.5s, or 0.25s before the bear resolution.

We consider three possible baseline correction all performed by subtracting the average pupil diameter from all subsequent values between, dividing those values by the average baseline, and averaging the baseline-corrected values vector.

This results in 24 datasets; 2 (plausible vs. implausible values) * 2 (on-screen vs. off-screen) * 2 (filtered or not) * 3 (1s, 0.5s, or 0.25s baseline).

```{r DF4 baseline correction, echo=TRUE, include=TRUE, out.width='90%'}
### 3 mean baselines, i.e., initial time window for each trial_num ,id, lab_id
### i.e. 1s, 0.5s, or 0.25s before resolution.
# MSS: According to the current data set, the resolution is at 0 ms for t_norm!

#!  There is currently an issue with the z-score baseline correct. The plots look weird!

baseline_correct <- function(data, TIME_WINDOW = 1000) {
  data |>
    group_by(participant_id, condition, outcome, lab_id) |>
    mutate(average = {
      avg_filtered <- average[t_norm >= -TIME_WINDOW & t_norm < 0]
      avg_mean <- mean(avg_filtered, na.rm = TRUE)
      (average - avg_mean)/avg_mean # Changed to calculate relative change.
    })
}
#
baseline_correctZ <- function(data, TIME_WINDOW = 1000) {
  data |>
    group_by(participant_id, condition, outcome, lab_id) |>
    mutate(average_z = {
      avg_filtered <- average_z[t_norm >= -TIME_WINDOW & t_norm < 0]
      avg_mean <- mean(avg_filtered, na.rm = TRUE)
      (average_z - avg_mean)/avg_mean # Changed to calculate relative change.
    })
}

data_baseline_1 <- data_df3 |> 
  mutate(data = map(data, partial(baseline_correct, TIME_WINDOW = 1000)),
         df4_baseline_correction = "1s", .before = "data") |>
    mutate(data = map(data, partial(baseline_correctZ, TIME_WINDOW = 1000))) # Z-score
#
data_baseline_0.5 <- data_df3 |>
  mutate(data = map(data, partial(baseline_correct, TIME_WINDOW = 500)),
         df4_baseline_correction = "0.5s", .before = "data") |>
  mutate(data = map(data, partial(baseline_correctZ, TIME_WINDOW = 500))) # Z-score
#
data_baseline_0.25 <- data_df3 |> 
  mutate(data = map(data, partial(baseline_correct, TIME_WINDOW = 250)),
         df4_baseline_correction = "0.25s", .before = "data") |>
    mutate(data = map(data, partial(baseline_correctZ, TIME_WINDOW = 250))) # Z-score
      
data_df4 <- bind_rows(
  data_baseline_1,
  data_baseline_0.5,
  data_baseline_0.25
)

# plotting
data_df4 |> 
  pull(data) |> 
  (`[`)(value = c(1:4)) |> 
  map(\(d) {
    ggplot(d |> filter(!is.na(average_z),
                       t_norm >= 0 & t_norm < 5000), 
           aes(x = t_norm, y = average_z, col = outcome, linetype = age_cohort)) +
      labs(x = "Time (ms)",
           y = "Pupil size (mm)") +
      geom_vline(xintercept = 0) +
      geom_hline(yintercept = 0) +
      geom_smooth(se = TRUE) +
      facet_grid(condition ~ age_cohort)
  })
```

# Fifth degree of freedom: Participant exclusion (following the criteria of MB2) at the level of the 1st vs 2nd trial. 

Here we use two strings to exclude participants who provided valid data only on the second test trial (remove_ids_1) or who provided valid data on both test trials (remove_ids_2).

This results in 72 datasets; 2 (plausible vs. implausible values) * 2 (on-screen vs. off-screen) * 2 (filtered or not) * 3 (1s, 0.5s, or 0.25s baseline) * 3 (no exclusions, valid trial 2 or valid trial 1 AND 2).

```{r DF5 participant exclusion, echo=TRUE, include=TRUE, out.width='90%'}
no_exclusions <- data_pupillometry |> 
  filter(pilot == "no",
      session_error == "noerror",
      age_exclusion == "no",
      sufficient_fam_trials == 1) |> 
  pull(participant_id) |> 
  unique()
valid_second_ids <- data_pupillometry |> 
  filter(pilot == "no",
      session_error == "noerror",
      age_exclusion == "no",
      sufficient_fam_trials == 1,
      valid_second_test_trial == 1) |> 
  pull(participant_id) |> 
  unique()
valid_both_ids <- data_pupillometry |> 
  filter(pilot == "no",
      session_error == "noerror",
      age_exclusion == "no",
      sufficient_fam_trials == 1,
      valid_second_test_trial == 1,
      valid_first_test_trial == 1) |> 
  pull(participant_id) |> 
  unique()

data_no_exclusions <- data_df4 |> 
  mutate(data = map(data, \(d) {
    d |> filter(participant_id %in% no_exclusions)
  }))
data_valid_second <- data_df4 |> 
  mutate(data = map(data, \(d) {
    d |> filter(participant_id %in% valid_second_ids)
  }))
data_valid_both <- data_df4 |> 
  mutate(data = map(data, \(d) {
    d |> filter(participant_id %in% valid_both_ids)
  }))

data_df5 <- bind_rows(
  data_no_exclusions |> mutate(df5_ppt_exclusion = "no exclusion", .before = "data"),
  data_valid_second |> mutate(df5_ppt_exclusion = "second test trial valid", .before = "data"),
  data_valid_both |> mutate(df5_ppt_exclusion = "both test trials valid", .before = "data")
)

```

```{r Ploting all 72 datasets and grand average, echo=TRUE, include=TRUE, out.width='90%'}
# work-around for different time points for now: rounding to the nearest 25ms
data_df5 <- data_df5 |> 
  mutate(data = map(data, ~ .x |> mutate(t_norm_downsampled = floor(t_norm / 25) * 25)))

# Generate and store all plots
plots <- data_df5 |> 
  pull(data) |> 
  map(\(d) {
    ggplot(d |> filter(!is.na(average),
                       t_norm >= 0 & t_norm < 5000), 
           aes(x = t_norm_downsampled, y = average, col = outcome, linetype = age_cohort)) +
      labs(x = "Time (ms)",
           y = "Pupil size (mm)") +
      geom_vline(xintercept = 0) +
      geom_hline(yintercept = 0) +
      geom_smooth(se = TRUE) +
      facet_grid(condition ~ age_cohort)
  })

# Extract labels for filenames
labels <- data_df5 |> 
  select(1:5) |>  # Select first 5 columns
  apply(1, paste, collapse = "_") # Combine column values into a string

# Add numbers before labels
labels <- paste0(seq_along(labels), "_", labels)

# Define output directory
output_dir <- "plots"
dir.create(output_dir, showWarnings = FALSE)

# Save each plot with a meaningful filename
walk2(plots, labels, ~ ggsave(
  filename = file.path(output_dir, paste0(.y, ".png")),
  plot = .x,
  width = 8, height = 6, dpi = 300
))

grand_avg <- data_df5 |> 
  pull(data) |> 
  bind_rows(.id = "dataset_id") |>  # Keep dataset ID
  filter(!is.na(average), t_norm_downsampled >= 0 & t_norm_downsampled < 5000) |>  
  group_by(t_norm_downsampled, condition, outcome, age_cohort) |>  
  summarize(
    grand_average = mean(average, na.rm = TRUE),  
    se = sd(average, na.rm = TRUE) / sqrt(n_distinct(dataset_id)),  # SE using # of datasets
    .groups = "drop"
  )

ggplot(grand_avg, 
       aes(x = t_norm_downsampled, y = grand_average, col = interaction(condition, outcome))) +
  labs(x = "Time (ms)", y = "Pupil size (mm)", color = "Condition & Outcome") +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_ribbon(aes(ymin = grand_average - se, ymax = grand_average + se, fill = interaction(condition, outcome)), 
              alpha = 0.25, color = NA) +
  geom_line() +
  facet_wrap(~ age_cohort)
```

As a final step, we investigate (1) the averaged interaction effect Condition x Outcome, (2) the time-course of the interaction effect Condition x Outcome and (3) the non-linear interaction effect of Condition x Outcome considering the time-course of the effect. This approach provides an exploration of whether and how smoothing time enhances the plausibility of statistical modeling of pupil dilation across datasets.

```{r modelling, echo=FALSE, include=TRUE, out.width='90%', message=FALSE}
# Assuming `data_df5$data` is a list of dataframes:
data_df5_list <- data_df5$data

# Run your model function on each dataframe in data_df5_list
# Define the custom modeling function
run_model_lm <- function(df) {
  if (is.data.frame(df) && n_distinct(df$condition_c) > 1 & 
      n_distinct(df$outcome_c) > 1 & n_distinct(df$age_cohort_c) > 1) {

    # Fit the model if conditions are met
    df.short <- df |>
      filter(t_norm > 0) |>
      group_by(participant_id, condition_c, outcome_c, age_cohort_c) |>
      summarize(Average = mean(average, na.rm=T)) |>
      ungroup()
    
    # Fit model.
    lm(Average ~ condition_c * outcome_c * age_cohort_c, data = df.short)
  } else {
    # Return NULL if conditions aren't met
    NULL
  }
}

run_model_lmer <- function(df) {
  if (is.data.frame(df) && n_distinct(df$condition_c) > 1 & 
      n_distinct(df$outcome_c) > 1 & n_distinct(df$age_cohort_c) > 1) {

    # Fit the model if conditions are met
    df.short <- df |>
      filter(t_norm > 0) |>
      group_by(participant_id, condition_c, outcome_c, age_cohort_c, lab_id) |>
      summarize(Average = mean(average, na.rm=T)) |>
      ungroup()
    
    lmer(Average ~ condition_c * outcome_c * age_cohort_c + (1|lab_id), data = df.short)
  } else {
    # Return NULL if conditions aren't met
    NULL
  }
}

run_model_TIMElmer <- function(df) {
  if (is.data.frame(df) && n_distinct(df$condition_c) > 1 & 
      n_distinct(df$outcome_c) > 1 & n_distinct(df$age_cohort_c) > 1) {
    # Fit the model if conditions are met
    
    #! Reduce dataset to to average across the 5 seconds, i.e., 1 value per participant per trial? 
    df.short <- df |>
      filter(t_norm > 0) |>
      group_by(participant_id, condition_c, t_norm, outcome_c, age_cohort_c, lab_id) |>
      summarize(Average = mean(average, na.rm=T)) |>
      ungroup()
    
    lmer(Average ~ t_norm*condition_c * outcome_c * age_cohort_c + (1|lab_id), data = df.short) #! Pending vote; include lab-id here. too! This is where we can include AL as fixed effect.
  } else {
    # Return NULL if conditions aren't met
    NULL
  }
}



# Apply the model function across the list of dataframes in the list-column
results_lm <- map(data_df5_list, run_model_lm)
results_lmer <- map(data_df5_list, run_model_lmer)
results_TIMElmer <- map(data_df5_list, run_model_TIMElmer)
# Filter out NULL values (dataframes that didn't meet the condition)
lm_results <- compact(results_lm)
lmer_results <- compact(results_lmer)
TIMElmer_results <- compact(results_TIMElmer)
```

```{r model parameters, echo=TRUE, include=TRUE, out.width='90%', message=FALSE}
# Define the extraction function with improved error handling
extract_model_metrics <- function(model) {
  if (!is.null(model)) {
    # Calculate R-squared
    rsq <- summary(model)$adj.r.squared
    tval <- summary(model)$coefficients[,3] 
    # Get AIC and BIC
    aic <- AIC(model)
    bic <- BIC(model)
    
    # Extract coefficients and standard errors (using tidy output)
#<<<<<<< Updated upstream
   coef_df <- broom::tidy(model) %>%
  filter(term == "condition_c:outcome_c:age_cohort_c") %>%  # Only the 3-way interaction
  select(term, estimate, std.error, p.value)
#=======
    coef_df <- broom::tidy(model)
    coef_df <- coef_df %>%
      filter(term %in% c("condition_c", "outcome_c", "age_cohort_c")) %>% 
      
      select(term, estimate, std.error)
##>>>>>>> Stashed changes
    
    # If coefficients are missing, create empty placeholders
    if (nrow(coef_df) == 0) {
      coef_df <- tibble(term = NA, estimate = NA, std.error = NA, p.value = NA)
    }
    
    # Add the AIC, BIC, and R-squared as additional columns for consistency
    coef_df <- coef_df %>%
      mutate(AIC = aic, BIC = bic, R_squared = rsq)
    
    return(coef_df)
  } else {
    NULL
  }
}

LMER_model_metrics <- map(lmer_results, function(model) {
  broom.mixed::tidy(model)
})

extract_model_metrics_LMER <- function(model) {
  r2 <- performance::r2(model)  # Extract R^2 values
  data.frame(
    AIC = AIC(model),
    BIC = BIC(model),
    logLik = logLik(model),
    marginal_R2 = r2$R2_marginal,   # Marginal R^2
    conditional_R2 = r2$R2_conditional,  # Conditional R^2
    fixed_effects = paste(names(fixef(model)), round(fixef(model), 3), collapse = ", ")
  )
}

timeLMER_model_metrics <- map(TIMElmer_results, function(model) {
  broom.mixed::tidy(model)
})

extract_model_metrics_timeLMER <- function(model) {
  r2 <- performance::r2(model)  # Extract R^2 values
  data.frame(
    AIC = AIC(model),
    BIC = BIC(model),
    logLik = logLik(model),
    marginal_R2 = r2$R2_marginal,   # Marginal R^2
    conditional_R2 = r2$R2_conditional,  # Conditional R^2
    fixed_effects = paste(names(fixef(model)), round(fixef(model), 3), collapse = ", ")
  )
}

# Apply the extraction function across all models and filter out NULL results
LM_model_metrics <- map(lm_results, extract_model_metrics)
LM_model_metrics <- compact(LM_model_metrics)  # Remove any NULL elements

LMER_model_metrics <- map(lmer_results, extract_model_metrics_LMER)
LMER_model_metrics <-compact(LMER_model_metrics) # Remove any NULL elements

timeLMER_model_metrics <- map(TIMElmer_results, extract_model_metrics_timeLMER)
timeLMER_model_metrics <- compact(LMER_model_metrics)  # Remove any 
# Convert to a tidy dataframe
LMmodel_metrics_df <- bind_rows(LM_model_metrics, .id = "model_id")
LMERmodel_metrics_df <- bind_rows(LMER_model_metrics, .id = "model_id")
timeLMERmodel_metrics_df <- bind_rows(timeLMER_model_metrics, .id = "model_id")


# Check the structure of model_metrics_df to confirm columns are present
str(LMmodel_metrics_df)
str(LMERmodel_metrics_df)
str(timeLMERmodel_metrics_df)
# Plot the specification curve

# Add a model order for plotting
LMmodel_metrics_df <- LMmodel_metrics_df %>%
  group_by(term) %>%
  mutate(model_order = row_number()) %>%
  ungroup() #|>
  #filter()

LMERmodel_metrics_df <- LMERmodel_metrics_df %>%
  group_by(fixed_effects) %>%
  mutate(model_order = row_number()) %>%
  ungroup()

timeLMERmodel_metrics_df <- timeLMERmodel_metrics_df %>%
  group_by(fixed_effects) %>%
  mutate(model_order = row_number()) %>%
  ungroup()

```

```{r model parameters plot, echo=TRUE, include=TRUE, out.width='90%'}
# Create the plot
# BIC LM
ggplot(LMmodel_metrics_df, aes(x = model_order, y = BIC, color = model_id)) +
  geom_point() +                                # Points for estimates
 # geom_errorbar(aes(ymin = BIC - std.error, ymax = BIC + std.error), 
            #    width = 0.2) +                  # Error bars
  #facet_wrap(~ term, scales = "free_y") +       # Separate panels for each predictor
  labs(title = "Specification Curve for BIC",
       x = "Model Order",
       y = "BIC") +
      # color = "Predictor") +
  theme_minimal()
#BIC LMER
ggplot(LMERmodel_metrics_df, aes(x = model_order, y = BIC, color = model_id)) +
  geom_point() +                                # Points for estimates
 # geom_errorbar(aes(ymin = BIC - std.error, ymax = BIC + std.error), 
            #    width = 0.2) +                  # Error bars
  #facet_wrap(~ term, scales = "free_y") +       # Separate panels for each predictor
  labs(title = "Specification Curve for BIC",
       x = "Model Order",
       y = "BIC") +
      # color = "Predictor") +
  theme_minimal() 

#BIC LMER time
ggplot(timeLMERmodel_metrics_df, aes(x = model_order, y = BIC, color = model_id)) +
  geom_point() +                                # Points for estimates
 # geom_errorbar(aes(ymin = BIC - std.error, ymax = BIC + std.error), 
            #    width = 0.2) +                  # Error bars
  #facet_wrap(~ term, scales = "free_y") +       # Separate panels for each predictor
  labs(title = "Specification Curve for BIC",
       x = "Model Order",
       y = "BIC") +
      # color = "Predictor") +
  theme_minimal() 


#AIC LM
ggplot(LMmodel_metrics_df, aes(x = model_order, y = AIC, color = model_id)) +
  geom_point() +                                # Points for estimates
 # geom_errorbar(aes(ymin = AIC - std.error, ymax = AIC + std.error), 
            #    width = 0.2) +                  # Error bars
  #facet_wrap(~ term, scales = "free_y") +       # Separate panels for each predictor
  labs(title = "Specification Curve for AIC",
       x = "Model Order",
       y = "AIC") +
      # color = "Predictor") +
  theme_minimal() 

#AIC LMER
ggplot(LMERmodel_metrics_df, aes(x = model_order, y = AIC, color = model_id)) +
  geom_point() +                                # Points for estimates
 # geom_errorbar(aes(ymin = AIC - std.error, ymax = AIC + std.error), 
            #    width = 0.2) +                  # Error bars
  #facet_wrap(~ term, scales = "free_y") +       # Separate panels for each predictor
  labs(title = "Specification Curve for AIC",
       x = "Model Order",
       y = "AIC") +
      # color = "Predictor") +
  theme_minimal()

#AIC LMER time
ggplot(LMERmodel_metrics_df, aes(x = model_order, y = AIC, color = model_id)) +
  geom_point() +                                # Points for estimates
 # geom_errorbar(aes(ymin = AIC - std.error, ymax = AIC + std.error), 
            #    width = 0.2) +                  # Error bars
  #facet_wrap(~ term, scales = "free_y") +       # Separate panels for each predictor
  labs(title = "Specification Curve for AIC",
       x = "Model Order",
       y = "AIC") +
      # color = "Predictor") +
  theme_minimal()


#R2 LM
ggplot(LMmodel_metrics_df, aes(x = model_order, y = R_squared, color = model_id)) +
  geom_point() +                                # Points for estimates
 # geom_errorbar(aes(ymin = R_squared - std.error, ymax = R_squared + std.error), 
            #    width = 0.2) +                  # Error bars
  facet_wrap(~ term, scales = "free_y") +       # Separate panels for each predictor
  labs(title = "Specification Curve for R_squared",
       x = "Model Order",
       y = "R_squared") +
      # color = "Predictor") +
  theme_minimal() 

#R2 CONDITIONAL LMER
ggplot(LMERmodel_metrics_df, aes(x = model_order, y = conditional_R2, color = model_id)) +
  geom_point() +                                # Points for estimates
 # geom_errorbar(aes(ymin = R_squared - std.error, ymax = R_squared + std.error), 
            #    width = 0.2) +                  # Error bars
#  facet_wrap(~ term, scales = "free_y") +       # Separate panels for each predictor
  labs(title = "Specification Curve for R_squared",
       x = "Model Order",
       y = "R_squared") +
      # color = "Predictor") +
  theme_minimal()

#R2 CONDITIONAL LMER time
ggplot(LMERmodel_metrics_df, aes(x = model_order, y = conditional_R2, color = model_id)) +
  geom_point() +                                # Points for estimates
 # geom_errorbar(aes(ymin = R_squared - std.error, ymax = R_squared + std.error), 
            #    width = 0.2) +                  # Error bars
#  facet_wrap(~ term, scales = "free_y") +       # Separate panels for each predictor
  labs(title = "Specification Curve for R_squared",
       x = "Model Order",
       y = "R_squared") +
      # color = "Predictor") +
  theme_minimal()


```

# Building a table to help look at the distribution of key statistics to address the following question: Would we have reached different conclusions given the dataset (as part of the multiverse)?

```{r table key parameters, echo=TRUE, include=TRUE, out.width='90%', message=FALSE}

# Start building the dataframe with key information already gathered in previous steps"
# The goal is to have all the information at hand to address the preregistered analyses (https://osf.io/wt79h)

# Merge info from LMER:
dataStats.temp <- data_df5 |>
  select(-"data")

dataStats.df <- LMERmodel_metrics_df |>
  select(-c("fixed_effects", "model_order")) |>
  rename(LMER.AIC = AIC, LMER.BIC = BIC, LMER.logLik = logLik, LMER.margR2 = marginal_R2, LMER.condR2 = conditional_R2)

dataStats.df <- bind_cols(dataStats.temp, dataStats.df) |>
  relocate(model_id)

rm("dataStats.temp")

# Merge info from LM:
dataStats.temp <- LMmodel_metrics_df |>
  filter(term == "condition_c") |>
  select(-c("term", "model_order", "model_id")) |>
  rename(LM.CondEstimate = estimate, LM.CondSE = std.error, LM.AIC = AIC, LM.BIC = BIC, LM.R2 = R_squared)

dataStats.df <- bind_cols(dataStats.df, dataStats.temp)

rm("dataStats.temp")

# Get more statistics for the df:

run_model_anova <- function(df) {
  
  if (is.data.frame(df) && n_distinct(df$condition_c) > 1 & 
      n_distinct(df$outcome_c) > 1 & n_distinct(df$age_cohort_c) > 1) {
      
    df.short <- df |>
      filter(t_norm > 0) |>
      group_by(participant_id, condition_c, outcome_c, age_cohort_c, condition, outcome, age_cohort) |>
      summarize(Average = mean(average, na.rm=T)) |>
      ungroup()
    
    # Hypothesis 1.
    # Fit model.
    df.modelFull <- lm(Average ~ condition_c * outcome_c * age_cohort_c, data = df.short)
    df.modelFull.sum <- summary(df.modelFull)
    
    # Add to df.
    df.return <- tibble(F.FullNull = df.modelFull.sum$fstatistic[1])
    
    # Get beta-coef for outcome given condition == knowledge and age_cohort == toddlers.
    df.short$condition <- as.factor(df.short$condition)
    df.short$condition <- relevel(df.short$condition, ref = "knowledge")
    df.short$age_cohort <- as.factor(df.short$age_cohort)
    df.short$age_cohort <- relevel(df.short$age_cohort, ref = "toddlers")
    
    df.modelFull2 <- lm(Average ~ condition * outcome * age_cohort, data = df.short)
    df.modelFull2 <- broom::tidy(df.modelFull2) |>
      filter(term=="outcomeincongruent") |>
      select(-c("statistic", "p.value"))
    
    df.return <- df.return |>
      mutate(Beta.FullCondition = as.numeric(df.modelFull2[2]), BetaSD.FullCondition = as.numeric(df.modelFull2[3]))
  
    ### 3-way interaction
    df.modelRedBy3Way <- lm(Average ~ (condition_c+outcome_c+age_cohort_c)^2, data = df.short)
    df.modelFullRed3Way <- anova(df.modelRedBy3Way, df.modelFull)
    
    Fvalue <- df.modelFullRed3Way$F[2]
    R2 <- summary(df.modelFull)$r.squared - summary(df.modelRedBy3Way)$r.squared
    
    df.return <- df.return |>
    mutate(F.FullRedBy3Way = Fvalue, R2.FullRedBy3Way = R2)
    rm("Fvalue", "R2")
    
    
    # Hypothesis 1.1 &  Hypothesis 1.2
    df.short <- df |>
      filter(t_norm > 0) |>
      group_by(participant_id, condition_c, outcome_c, age_cohort_c) |>
      summarize(Average = mean(average, na.rm=T)) |>
      ungroup()
    #
    df.modelFull <- lm(Average ~ condition_c * outcome_c * age_cohort_c, data = df.short)
    df.modelRedByOutome <- lm(Average ~ condition_c * age_cohort_c, data = df.short)
    df.modelFullRedOutcome <- anova(df.modelRedByOutome, df.modelFull)
    
    Fvalue <- df.modelFullRedOutcome$F[2]
    R2 <- summary(df.modelFull)$r.squared - summary(df.modelRedByOutome)$r.squared
    
    df.return <- df.return |>
    mutate(F.FullRedByOutcome = Fvalue, R2.FullRedByOutcome = R2)
    rm("Fvalue", "R2")
    
    # Hypothesis 1.3 &  Hypothesis 1.4
    df.modelRedByCondition <- lm(Average ~ outcome_c * age_cohort_c, data = df.short)
    df.modelFullRedCondition <- anova(df.modelRedByCondition, df.modelFull)
    
    Fvalue <- df.modelFullRedCondition$F[2]
    R2 <- summary(df.modelFull)$r.squared - summary(df.modelRedByCondition)$r.squared
    
    df.return <- df.return |>
    mutate(F.FullRedByCondition = Fvalue, R2.FullRedByCondition = R2)
    rm("Fvalue", "R2")
    
    return(df.return)
  } else {
    # Return NULL if conditions aren't met
    NULL
  }
}

dataStats.df.additional <- map(data_df5_list, run_model_anova)
dataStats.df.additional <- bind_rows(dataStats.df.additional, .id = "model_id")

dataStats.final <- inner_join(dataStats.df, dataStats.df.additional, by = "model_id")

hist(dataStats.final$F.FullNull)
#
plot(sort(dataStats.final$F.FullNull), ylim=c(0,8))
points(sort(dataStats.final$F.FullRedByOutcome), pch=19, col="blue")
points(sort(dataStats.final$F.FullRedByCondition), pch=19, col="green")
points(sort(dataStats.final$F.FullRedBy3Way), pch=19, col="red")
legend("topleft",                             # Position of legend
       legend = c("Full Null", 
                  "Full Reduced by Outcome", 
                  "Full Reduced by Condition", 
                  "Full Reduced by 3-Way"),   # Labels
       col = c("black", "blue", "green", "red"), # Colors to match your points
       pch = c(1,19,19,19),                   # Point styles matching plot
       bty = "n")    

plot(sort(dataStats.final$Beta.FullCondition), pch=19, ylim =c(-0.01, 0.01), ylab="Beta-coeff from congruent to incongruent", main="Change in relative Pupil Size for\n Toddlers in the Knowledge Condition")
abline(h=0)

# Spot check
dataStats.final |>
  relocate(F.FullNull)
#
dataStats.final |>
  relocate(LMER.AIC)

# It looks like all the model values are identical with filtered datasets (here no effect of df1 and df2?)? Fixed by applying filter to average (see above)
# Likewise, within unfiltered, the two plausible are identical and the two implausible are identical. Fixed by applying filter to average (see above)
```





