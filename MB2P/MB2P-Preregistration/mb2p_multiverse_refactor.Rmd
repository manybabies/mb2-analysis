---
title: "MB2-P - Multiverse Data Simulation and Statistical Analysis"
author: "Rmarkdown by Giulia Calignano, Marlena Mayer, Robert Hepach "
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    css: styles.css
  pdf_document: default
  word_document: default
  theme: united
---

```{r setup, include=FALSE}
# Clear workspace:
rm(list = ls())

knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(zoo)
library(lme4)
library(mgcv)
library(here)
library(dplyr)
library(tidyverse)
library(stringi)
library(gridExtra)
```

```{r setup2, include=FALSE}
set.seed(123) # for reproducibility
source(here("helper", "ensure_repo_structure.R"))
theme_set(theme_classic())

# load pupillometry data
load(here(INTERMEDIATE_FOLDER, INTERMEDIATE_008))
```

## The Data needed

- **participant_id**: A unique identifier for each participant, generated as a random string. Each participant has a unique ID to distinguish them in the dataset.
- **age_cohort**: A randomly generated string intended to categorize participants into different age cohorts.
- **t**: A numeric variable representing a timestamp or duration, generated within a specified range and with a possibility of missing values (NA).
- **x and y**: Numeric variables generated within specified ranges, intended to represent coordinates or other measurements, with a possibility of missing values.
- **pupil_left and pupil_right**: Numeric variables representing measurements of the left and right pupil sizes, respectively. These variables are generated with a specified correlation and then rescaled to a specific range.
- **lab_id**: A randomly generated string serving as a unique identifier for the lab or testing location.
- **conditions**: A categorical variable with two levels, i.e. "knowledge" and "ignorance"
- **outcomes**: A categorical variable with two levels, i.e. "incongruent" and "congruent"

## The Multiverse forking paths of pupillometry preprocessing 

The following script specifies the preprocessing and analysis steps of the multiverse approach applied to the MB2-P (see Calignano, Girardi, and Alto√©, 2023).

Let's assume that we have 10s worth of data per participant which starts at 1s before the resolution until 9s after the resolution. The crucial time-window for our analyses is the 5s after the bear exits the tubes, i.e,. from 1s to 6s in this dataset.

```{r preprocess}
# Check Nijmegen-lab
#data.ni <- data_pupillometry |> 
#  filter(lab_id == "babylabNijmegen",
#  trial_num == 6,                   
#  t_norm >= -1000 & t_norm < 5000)
# hist(data.ni$pupil_right)
# head(data.ni$pupil_left)
# -> Looks like values need to be divided by 100 to transform to mm.

data_pupillometry <- data_pupillometry |> 
  filter(#eyetracker_type == "Tobii",       # pending resolution for Eyelink
         #lab_id != "babylabNijmegen",      # Nijmegen has pupil values in the 100s
         trial_num == 6,                   # filter to second test trial only (data reduction)
         t_norm >= -1000 & t_norm < 5000)  # filter to window of interest only (data reduction)

data_pupillometry <- data_pupillometry |> 
  mutate(age_cohort <- as.factor(age_cohort)) |> 
  rowwise() |> 
  mutate(average = mean(c(pupil_left, pupil_right), na.rm = TRUE)) |> # switched to liberal avg
  ungroup()

# Merging in AL data (variable prop_exit)

summarize_participant_test_both_trials <- readRDS(here(RESULTS_FOLDER,"summarize_participant_test_both_trials.rds"))

data_pupillometry <- data_pupillometry %>% left_join(summarize_participant_test_both_trials %>% dplyr::select("participant_lab_id", "trial_num", "prop_exit"), by = c( "participant_lab_id", "trial_num"))

rm(summarize_participant_test_both_trials)
```

# First Degree of freedom: Filtering of extreme yet plausible pupil values, i.e.<2mm, >8mm  

```{r DF1 extreme tonic values, echo=TRUE, include=TRUE, out.width='90%'}
data_pupillometry <- data_pupillometry |> 
  mutate(pupil_left = ifelse(pupil_left < 0, NA, pupil_left),
         pupil_right = ifelse(pupil_right < 0, NA, pupil_right))

### The following is a temporary workaround to include the Eyelink-data by calculating those values which would correspond to 2mm and 8mm, i.e., the mm-values for the first degree of freedom in the multiverse.

data_plausible.SMI.Tobii <- data_pupillometry |> 
  filter(eyetracker_type != "EyeLink")
#
pupil.vector <- data_plausible.SMI.Tobii[!is.na(data_plausible.SMI.Tobii$average),]$average
sum(pupil.vector<=2)/length(pupil.vector)
sum(pupil.vector>8)/length(pupil.vector)
range(pupil.vector)
quantile(pupil.vector)
# -> 2mm and 8mm correspond to 0.001568934 and 1
data_plausible.Eyelink <- data_pupillometry |> 
  filter(eyetracker_type == "EyeLink")
#
pupil.vector <- data_plausible.Eyelink[!is.na(data_plausible.Eyelink$average),]$average
range(pupil.vector)
quantile(pupil.vector, probs=c(0.001568934, 1))
# -> corresponds to 52 and 3008

###

# plausible data range ]2, 8[ for SMI and Tobii
data_plausible.SMI.Tobii <- data_pupillometry |> 
  filter(eyetracker_type != "EyeLink") |>
  mutate(average = ifelse(average < 2 | average > 8, NA, average))

# plausible data range ]52, 3008[ for Eyelink
data_plausible.Eyelink <- data_pupillometry |> 
  filter(eyetracker_type == "EyeLink") |>
    mutate(average = ifelse(average < 52 | average > 3008, NA, average))


data_plausible <- bind_rows(data_plausible.SMI.Tobii, data_plausible.Eyelink)

data_df1 <- bind_rows(
  data_pupillometry |> 
    mutate(df1_extreme_values = "implausible"),
  data_plausible |> 
    mutate(df1_extreme_values = "plausible")
) |> nest(data = -df1_extreme_values)

# scatterplot
data_pupillometry |> 
  filter(eyetracker_type != "EyeLink") |>
  mutate(plausibility = ifelse(pupil_left <= 2 | pupil_left >= 8 | 
                                 pupil_right <= 2 | pupil_right >= 8,
                               "implausible", "plausible")) |> 
  ggplot(aes(x = pupil_left, y = pupil_right, col = plausibility)) +
  geom_point(alpha = .1) +
  labs(x = "Pupil size left eye (mm)",
       y = "Pupil size right eye (mm)") +
  scale_color_manual(values = c("indianred", "royalblue")) +
  facet_wrap("age_cohort")
```

# Second degree of freedom: fixation within the screen vs outside the screen

```{r DF2 area of interest, echo=TRUE, include=TRUE, out.width='90%'}
data_within <- data_df1 |> 
  mutate(data = map(data, \(d) {
    d |> 
      mutate(x = ifelse(x <= 0 | x >= 1280, NA, x),
             y = ifelse(y <= 0 | y >= 960, NA, y))
  }))

data_df2 <- bind_rows(
  data_df1 |> mutate(df2_screen_fixation = "outside", .before = "data"),
  data_within |> mutate(df2_screen_fixation = "within", .before = "data")
)

data_pupillometry |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .1)
```
# Third degree of freedom: moving average filtered vs unfiltered data

```{r DF3 moving average, echo=TRUE, include=TRUE, out.width='90%'}
WINDOW_SIZE = 5

data_avg <- data_df2 |> 
  mutate(data = map(data, \(d) {
    d |> 
      group_by(participant_id, trial_num) |> 
      mutate(pupil_left = rollapply(pupil_left, width = WINDOW_SIZE, 
                                    FUN = mean, na.rm = TRUE, 
                                    fill = NA, align = "right"),
             pupil_right = rollapply(pupil_right, width = WINDOW_SIZE, 
                                     FUN = mean, na.rm = TRUE, 
                                     fill = NA, align = "right")) |> 
      rowwise() |> 
      mutate(average = mean(c(pupil_left, pupil_right), na.rm = TRUE)) |> # NEW: need to update average after applying rolling average; switched to liberal avg
      ungroup() 
  }))

data_df3 <- bind_rows(
  data_df2 |> mutate(df3_moving_average = "unfiltered", .before = "data"),
  data_avg |> mutate(df3_moving_average = "filtered", .before = "data")
)
```

# Fourth degree of freedom: 1s, 0.5s, or 0.25s before the bear resolution.

We consider three possible baseline correction all performed by subtracting the average pupil diameter from all subsequent values between, dividing those values by the average baseline, and averaging the baseline-corrected values vector.

```{r DF4 baseline correction, echo=TRUE, include=TRUE, out.width='90%'}
### 3 median baselines, i.e., initial time window for each trial_num ,id, lab_id
### i.e. 1s, 0.5s, or 0.25s before resolution.
# MSS: According to the current data set, the resolution is at 0 ms for t_norm!
# FIXME: note suggests median, but actual code uses mean
baseline_correct <- function(data, TIME_WINDOW = 1000) {
  data |> 
    group_by(participant_id, condition, outcome, lab_id) |> 
    mutate(average = {
      avg_filtered <- average[t_norm >= -TIME_WINDOW & t_norm < 0]
      avg_mean <- mean(avg_filtered, na.rm = TRUE)
      (average - avg_mean)/avg_mean # Changed to calculate relative change.
    })
}

data_baseline_1 <- data_df3 |> 
  mutate(data = map(data, partial(baseline_correct, TIME_WINDOW = 1000)),
         df4_baseline_correction = "1s", .before = "data")
data_baseline_0.5 <- data_df3 |> 
  mutate(data = map(data, partial(baseline_correct, TIME_WINDOW = 500)),
         df4_baseline_correction = "0.5s", .before = "data")
data_baseline_0.25 <- data_df3 |> 
  mutate(data = map(data, partial(baseline_correct, TIME_WINDOW = 250)),
         df4_baseline_correction = "0.25s", .before = "data")

data_df4 <- bind_rows(
  data_baseline_1,
  data_baseline_0.5,
  data_baseline_0.25
)

# plotting
# RH: Changed the color and facet to show condition within panel.

data_df4 |> 
  pull(data) |> 
  (`[`)(value = c(2,7)) |> # arbitrarily select two universes to visualise
  map(\(d) {
    ggplot(d |> filter(!is.na(average),
                       t_norm >= 0 & t_norm < 5000), 
           aes(x = t_norm, y = average, col = outcome, linetype = age_cohort)) +
      labs(x = "Time (ms)",
           y = "Pupil size (mm)") +
      geom_vline(xintercept = 0) +
      geom_hline(yintercept = 0) +
      geom_smooth(se = TRUE) +
      facet_grid(condition ~ eyetracker_type)
  })
```

# Fifth degree of freedom: Participant exclusion (following the criteria of MB2) at the level of the 1st vs 2nd trial. 

Here we use two strings to exclude participants who provided valid data only on the second test trial (remove_ids_1) or who provided valid data on both test trials (remove_ids_2).

```{r DF5 participant exclusion, echo=TRUE, include=TRUE, out.width='90%'}

no_exclusions <- data_pupillometry |> 
  filter(pilot == "no",
      session_error == "noerror",
      age_exclusion == "no",
      sufficient_fam_trials == 1) |> 
  pull(participant_lab_id) |> 
  unique()
valid_second_ids <- data_pupillometry |> 
  filter(pilot == "no",
      session_error == "noerror",
      age_exclusion == "no",
      sufficient_fam_trials == 1,
      valid_second_test_trial == 1) |> 
  pull(participant_lab_id) |> 
  unique()
valid_both_ids <- data_pupillometry |> 
  filter(pilot == "no",
      session_error == "noerror",
      age_exclusion == "no",
      sufficient_fam_trials == 1,
      valid_second_test_trial == 1,
      valid_first_test_trial == 1) |> 
  pull(participant_lab_id) |> 
  unique()

data_no_exclusions <- data_df4 |> 
  mutate(data = map(data, \(d) {
    d |> filter(participant_lab_id %in% no_exclusions)
  }))
data_valid_second <- data_df4 |> 
  mutate(data = map(data, \(d) {
    d |> filter(participant_lab_id %in% valid_second_ids)
  }))
data_valid_both <- data_df4 |> 
  mutate(data = map(data, \(d) {
    d |> filter(participant_lab_id %in% valid_both_ids)
  }))

data_df5 <- bind_rows(
  data_no_exclusions |> mutate(df5_ppt_exclusion = "no exclusion", .before = "data"),
  data_valid_second |> mutate(df5_ppt_exclusion = "second test trial valid", .before = "data"),
  data_valid_both |> mutate(df5_ppt_exclusion = "both test trials valid", .before = "data")
)
```
At this point we have an object called megaverse. The megaverse is a comprehensive collection of datasets organized into three different versions, each known as a multiverse. Each multiverse represents a different scenario or variation of the original dataset. Here's a detailed description of the structure:

Original Multiverse (multiverse_one): This version contains the unaltered original data. It serves as the baseline for comparison with other multiverses.
Modified Multiverse 1 (multiverse_two): In this version, certain participants are removed based on a specified list of participant IDs (remove_ids_1). This allows for analyzing how the removal of specific participants affects the results.
Modified Multiverse 2 (multiverse_three): This version excludes a different set of participants, defined by another list of participant IDs (remove_ids_2). This provides another perspective on the dataset with a different subset of participants removed.
Each multiverse consists of:

A list of lists, where each sublist contains one or more data frames.
Each data frame represents a segment of the dataset, including columns such as participant_id and other relevant variables included in the simulated data.
By structuring the data in this way, the megaverse allows for systematic comparison across different scenarios, facilitating a robust analysis of the dataset under various conditions.

<br>

As a final step, we investigated (1) the averaged interaction effect Condition x Outcome, (2) the time-course of the interaction effect Condition x Outcome and (3) the non linear interaction effect of Condition x Outcome considering the time-course of the effect. This approach provided an exploration of whether and how smoothing time enhances the plausibility of statistical modeling of pupil dilation across the datasets we created.

For the analysis of pupillary data, we utilized generalized additive mixed modeling (GAMM, Wood, 2011). GAMMs combine the flexibility of generalized additive models (GAMs) with the ability to incorporate random effects, which are essential for accounting for correlations among observations within clusters or groups. This includes the use of smooth functions that handle both continuous and categorical predictors. The random effects component facilitates the inclusion of hierarchical structures, such as nested or repeated measures within the data, making GAMMs useful for a wide range of data types, including time series, spatial, and longitudinal data. The model is estimated using penalized regression techniques, which help to avoid overfitting and produce more reliable predictions.

```{r modelling, echo=TRUE, include=TRUE, out.width='90%'}
data_analysis <- data_df5 |> 
  mutate(
    model_lm = map(data, \(d) {
      lm(average ~ condition * outcome * age_cohort, 
         data = d |> 
           filter(t_norm > 0 & t_norm < 5000) |> 
           group_by(participant_id, condition, outcome, age_cohort) |> 
           summarise(average = mean(average, na.rm = TRUE),
                     .groups = "drop"))
    }),
    model_lmer = map(data, \(d) {
      lmer(average ~ condition * outcome * age_cohort * t_norm +
             (1 + t | participant_id),
           data = d)
    }),
    model_gam = map(data, \(d) {
      bam(average ~ condition * outcome * age_cohort +
            s(t_norm, by = lab_id, k = 7) +
            s(t_norm, by = condition, k = 20) +
            s(t_norm, by = outcome, k = 20) +
            s(t_norm, participant_id, bs = "fs", m = 1),
          data = d |> 
            mutate(lab_id = as.factor(lab_id),
                   condition = as.factor(condition),
                   outcome = as.factor(outcome),
                   participant_id = as.factor(participant_id)), 
          discrete = TRUE, 
          nthreads = 40)
    }))
```
To select the best model based on the lowest BIC, lowest AIC, and highest R-squared (Calignano et al., 2024), we computed the aboved cited statistics for each model and then compare them. We create a function to extract these metrics, identify the best model based on each criterion, and then plot the effects of the best model.

```{r, MODEL SELECTION, echo=TRUE,include=TRUE, out.width='90%'}
# Function to extract BIC, AIC, and R-squared from a model
extract_model_metrics <- function(model) {
  if (inherits(model, "lm") || inherits(model, "lmerMod") || inherits(model, "bam")) {
    aic_value <- AIC(model)
    bic_value <- BIC(model)
    r_squared <- if (inherits(model, "lm")) {
      summary(model)$r.squared
    } else if (inherits(model, "lmerMod")) {
      r.squaredGLMM(model)[1] # Marginal R-squared for mixed models
    } else if (inherits(model, "bam")) {
      summary(model)$r.sq
    } else {
      NA
    }
    return(list(AIC = aic_value, BIC = bic_value, R_squared = r_squared))
  } else {
    return(NULL)
  }
}

# Function to compare models and select the best based on AIC, BIC, and R-squared
compare_models <- function(models_list) {
  model_metrics <- map(models_list, ~ map(.x, extract_model_metrics))
  
  best_models <- list()
  best_models$AIC <- model_metrics |>
    map_df(~ map_df(.x, ~ data.frame(AIC = .x$AIC)), .id = "model") |>
    arrange(AIC) |>
    slice(1)
  
  best_models$BIC <- model_metrics |>
    map_df(~ map_df(.x, ~ data.frame(BIC = .x$BIC)), .id = "model") |>
    arrange(BIC) |>
    slice(1)
  
  best_models$R_squared <- model_metrics |>
    map_df(~ map_df(.x, ~ data.frame(R_squared = .x$R_squared)), .id = "model") |>
    arrange(desc(R_squared)) |>
    slice(1)
  
  return(best_models)
}

library(ggplot2)

# Function to plot the effects of a model
plot_model_effects <- function(model) {
  if (inherits(model, "lm")) {
    plot_data <- data.frame(
      Fitted = fitted(model),
      Residuals = residuals(model)
    )
    ggplot(plot_data, aes(Fitted, Residuals)) +
      geom_point() +
      geom_smooth(method = "loess") +
      labs(title = "Effects of the Best Linear Model")
  } else if (inherits(model, "lmerMod")) {
    plot_data <- data.frame(
      Fitted = fitted(model),
      Residuals = residuals(model)
    )
    ggplot(plot_data, aes(Fitted, Residuals)) +
      geom_point() +
      geom_smooth(method = "loess") +
      labs(title = "Effects of the Best Linear Mixed Model")
  } else if (inherits(model, "bam")) {
    plot(model, pages = 1, main = "Effects of the Best Generalized Additive Model")
  } else {
    stop("Unknown model type")
  }
}
```
