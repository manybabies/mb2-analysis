---
title: "MB2-P - Multiverse Data Simulation and Statistical Analysis"
author: "Rmarkdown by Giulia Calignano, Marlena Mayer, Robert Hepach "
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    css: styles.css
  pdf_document: default
  word_document: default
  theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(zoo)
library(here)
library(tidyverse)
library(stringi)
library(gridExtra)
```

```{r setup2, include=FALSE}
set.seed(123) # for reproducibility
source(here("helper", "ensure_repo_structure.R"))
theme_set(theme_classic())

# load pupillometry data
load(here(INTERMEDIATE_FOLDER, INTERMEDIATE_008))
```

## The Data needed

- **participant_id**: A unique identifier for each participant, generated as a random string. Each participant has a unique ID to distinguish them in the dataset.
- **age_cohort**: A randomly generated string intended to categorize participants into different age cohorts.
- **t**: A numeric variable representing a timestamp or duration, generated within a specified range and with a possibility of missing values (NA).
- **x and y**: Numeric variables generated within specified ranges, intended to represent coordinates or other measurements, with a possibility of missing values.
- **pupil_left and pupil_right**: Numeric variables representing measurements of the left and right pupil sizes, respectively. These variables are generated with a specified correlation and then rescaled to a specific range.
- **lab_id**: A randomly generated string serving as a unique identifier for the lab or testing location.
- **conditions**: A categorical variable with two levels, i.e. "knowledge" and "ignorance"
- **outcomes**: A categorical variable with two levels, i.e. "incongruent" and "congruent"

## The Multiverse forking paths of pupillometry preprocessing 

The following script specifies the preprocessing and analysis steps of the multiverse approach applied to the MB2-P (see Calignano, Girardi, and Alto√©, 2023).

Let's assume that we have 10s worth of data per participant which starts at 1s before the resolution until 9s after the resolution. The crucial time-window for our analyses is the 5s after the bear exits the tubes, i.e,. from 1s to 6s in this dataset.

# First Degree of freedom: Filtering of extreme yet plausible pupil values, i.e.<2mm, >8mm  

```{r preprocess}
data_pupillometry <- data_pupillometry |> 
  filter(eyetracker_type == "Tobii",       # pending resolution for Eyelink
         lab_id != "babylabNijmegen",      # Nijmegen has pupil values in the 100s
         trial_num == 6,                   # filter to second test trial only (data reduction)
         t_norm >= -1000 & t_norm < 5000)  # filter to window of interest only (data reduction)

data_pupillometry <- data_pupillometry |> 
  mutate(age_cohort <- as.factor(age_cohort),
         average = (pupil_left + pupil_right) / 2)
```


```{r DF1 extreme tonic values, echo=TRUE, include=TRUE, out.width='90%'}
data_pupillometry <- data_pupillometry |> 
  mutate(pupil_left = ifelse(pupil_left < 0, NA, pupil_left),
         pupil_right = ifelse(pupil_right < 0, NA, pupil_right))

# plausible data range ]2, 8[
data_plausible <- data_pupillometry |> 
  mutate(average = ifelse(average < 2 | average > 8, NA, average))

data_df1 <- bind_rows(
  data_pupillometry |> 
    mutate(df1_extreme_values = "implausible"),
  data_plausible |> 
    mutate(df1_extreme_values = "plausible")
) |> nest(data = -df1_extreme_values)

# scatterplot
data_pupillometry |> 
  mutate(plausibility = ifelse(pupil_left <= 2 | pupil_left >= 8 | 
                                 pupil_right <= 2 | pupil_right >= 8,
                               "implausible", "plausible")) |> 
  ggplot(aes(x = pupil_left, y = pupil_right, col = plausibility)) +
  geom_point(alpha = .1) +
  labs(x = "Pupil size left eye (mm)",
       y = "Pupil size right eye (mm)") +
  scale_color_manual(values = c("indianred", "royalblue")) +
  facet_wrap("age_cohort")
```

# Second degree of freedom: fixation within the screen vs outside the screen

```{r DF2 area of interest, echo=TRUE, include=TRUE, out.width='90%'}
data_within <- data_df1 |> 
  mutate(data = lapply(data, \(d) {
    d |> 
      mutate(x = ifelse(x <= 0 | x >= 1280, NA, x),
             y = ifelse(y <= 0 | y >= 960, NA, y))
  }))

data_df2 <- bind_rows(
  data_df1 |> mutate(df2_screen_fixation = "outside", .before = "data"),
  data_within |> mutate(df2_screen_fixation = "within", .before = "data")
)

data_pupillometry |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .1)
```
# Third degree of freedom: moving average filtered vs unfiltered data

```{r DF3 moving average, echo=TRUE, include=TRUE, out.width='90%'}
WINDOW_SIZE = 5

data_avg <- data_df2 |> 
  mutate(data = lapply(data, \(d) {
    d |> 
      group_by(participant_id, trial_num) |> 
      mutate(pupil_left = rollapply(pupil_left, width = WINDOW_SIZE, 
                                    FUN = mean, na.rm = TRUE, 
                                    fill = NA, align = "right"),
             pupil_right = rollapply(pupil_right, width = WINDOW_SIZE, 
                                     FUN = mean, na.rm = TRUE, 
                                     fill = NA, align = "right"),
             average = (pupil_left + pupil_right) / 2) |> # NEW: need to update average after applying rolling average
      ungroup()
  }))

data_df3 <- bind_rows(
  data_df2 |> mutate(df3_moving_average = "unfiltered", .before = "data"),
  data_avg |> mutate(df3_moving_average = "filtered", .before = "data")
)
```

# Fourth degree of freedom: 1s, 0.5s, or 0.25s before the bear resolution.

We consider three possible baseline correction all performed by subtracting the average pupil diameter from all subsequent values between, dividing those values by the average baseline, and averaging the baseline-corrected values vector.

```{r DF4 baseline correction, echo=TRUE, include=TRUE, out.width='90%'}
### 3 median baselines, i.e., initial time window for each trial_num ,id, lab_id
### i.e. 1s, 0.5s, or 0.25s before resolution.
# MSS: According to the current data set, the resolution is at 0 ms for t_norm!
# FIXME: note suggests median, but actual code uses mean
baseline_correct <- function(data, TIME_WINDOW = 1000) {
  data |> 
      group_by(participant_id, condition, outcome, lab_id) |> 
      mutate(average = {
        avg_filtered <- average[t_norm >= -TIME_WINDOW & t_norm < 0]
        avg_mean <- mean(avg_filtered, na.rm = TRUE)
        average - avg_mean
      })
}

data_baseline_1 <- data_df3 |> 
  mutate(data = lapply(data, partial(baseline_correct, TIME_WINDOW = 1000)),
         df4_baseline_correction = "1s", .before = "data")
data_baseline_0.5 <- data_df3 |> 
  mutate(data = lapply(data, partial(baseline_correct, TIME_WINDOW = 500)),
         df4_baseline_correction = "0.5s", .before = "data")
data_baseline_0.25 <- data_df3 |> 
  mutate(data = lapply(data, partial(baseline_correct, TIME_WINDOW = 250)),
         df4_baseline_correction = "0.25s", .before = "data")

data_df4 <- bind_rows(
  data_baseline_1,
  data_baseline_0.5,
  data_baseline_0.25
)

# plotting
data_df4 |> 
  pull(data) |> 
  (`[`)(value = c(2,7)) |> # arbitrarily select two universes to visualise
  lapply(\(d) {
    ggplot(d |> filter(!is.na(average),
                       t_norm >= 0 & t_norm < 5000), 
           aes(x = t_norm, y = average, col = condition)) +
      labs(x = "Time (ms)",
           y = "Pupil size (mm)") +
      geom_vline(xintercept = 0) +
      geom_hline(yintercept = 0) +
      geom_smooth(se = TRUE) +
      facet_wrap(outcome ~ age_cohort)
  })
```

# Fifth degree of freedom: Participant exclusion (following the criteria of MB2) at the level of the 1st vs 2nd trial. 

Here we use two strings to exclude participants who provided valid data only on the second test trial (remove_ids_1) or who provided valid data on both test trials (remove_ids_2).

```{r DF5 participant exclusion, echo=TRUE, include=TRUE, out.width='90%'}
valid_second_ids <- data_pupillometry |> 
  filter(valid_second_test_trial == 1) |> 
  pull(participant_lab_id) |> 
  unique()
valid_both_ids <- data_pupillometry |> 
  filter(valid_second_test_trial == 1,
         valid_first_test_trial == 1) |> 
  pull(participant_lab_id) |> 
  unique()

data_valid_second <- data_df4 |> 
  mutate(data = lapply(data, \(d) {
    d |> filter(participant_lab_id %in% valid_second_ids)
  }))
data_valid_both <- data_df4 |> 
  mutate(data = lapply(data, \(d) {
    d |> filter(participant_lab_id %in% valid_both_ids)
  }))

data_df5 <- bind_rows(
  data_df4 |> mutate(df5_ppt_exclusion = "no exclusion", .before = "data"),
  data_valid_second |> mutate(df5_ppt_exclusion = "second test trial valid", .before = "data"),
  data_valid_both |> mutate(df5_ppt_exclusion = "both test trials valid", .before = "data")
)
```
At this point we have an object called megaverse. The megaverse is a comprehensive collection of datasets organized into three different versions, each known as a multiverse. Each multiverse represents a different scenario or variation of the original dataset. Here's a detailed description of the structure:

Original Multiverse (multiverse_one): This version contains the unaltered original data. It serves as the baseline for comparison with other multiverses.
Modified Multiverse 1 (multiverse_two): In this version, certain participants are removed based on a specified list of participant IDs (remove_ids_1). This allows for analyzing how the removal of specific participants affects the results.
Modified Multiverse 2 (multiverse_three): This version excludes a different set of participants, defined by another list of participant IDs (remove_ids_2). This provides another perspective on the dataset with a different subset of participants removed.
Each multiverse consists of:

A list of lists, where each sublist contains one or more data frames.
Each data frame represents a segment of the dataset, including columns such as participant_id and other relevant variables included in the simulated data.
By structuring the data in this way, the megaverse allows for systematic comparison across different scenarios, facilitating a robust analysis of the dataset under various conditions.

<br>

As a final step, we investigated (1) the averaged interaction effect Condition x Outcome, (2) the time-course of the interaction effect Condition x Outcome and (3) the non linear interaction effect of Condition x Outcome considering the time-course of the effect. This approach provided an exploration of whether and how smoothing time enhances the plausibility of statistical modeling of pupil dilation across the datasets we created.

For the analysis of pupillary data, we utilized generalized additive mixed modeling (GAMM, Wood, 2011). GAMMs combine the flexibility of generalized additive models (GAMs) with the ability to incorporate random effects, which are essential for accounting for correlations among observations within clusters or groups. This includes the use of smooth functions that handle both continuous and categorical predictors. The random effects component facilitates the inclusion of hierarchical structures, such as nested or repeated measures within the data, making GAMMs useful for a wide range of data types, including time series, spatial, and longitudinal data. The model is estimated using penalized regression techniques, which help to avoid overfitting and produce more reliable predictions.

```{r, SETTING MODEL, FUNCTIONS, echo=TRUE,include=TRUE, out.width='90%'}
# Inspect the structure of megaverse
str(megaverse, max.level = 3)

extract_data_frame <- function(data) {
  while (is.list(data) && length(data) == 1) {
    data <- data[[1]]
  }
  return(data)
}

library(dplyr)
library(lme4)
library(mgcv)
library(purrr)

# Function to check if all dataset are a data frame
is_valid_dataset <- function(data) {
  return(is.data.frame(data))
}

# Pre-validation: Validate all datasets in megaverse with debug information
validate_megaverse <- function(megaverse) {
  validation_results <- map2(megaverse, seq_along(megaverse), function(sublist, i) {
    if (is.list(sublist)) {
      return(map2(sublist, seq_along(sublist), function(innerlist, j) {
        if (is.list(innerlist)) {
          return(map2(innerlist, seq_along(innerlist), function(innermostlist, k) {
            if (is.list(innermostlist)) {
              return(map2(innermostlist, seq_along(innermostlist), function(dataset, l) {
                dataset <- extract_data_frame(dataset)
                cat(sprintf("Validating dataset at [%d][%d][%d][%d]: Type = %s\n", i, j, k, l, class(dataset)))
                if (!is_valid_dataset(dataset)) {
                  warning(sprintf("Dataset at [%d][%d][%d][%d] is not a valid data frame. Type: %s", i, j, k, l, class(dataset)))
                  return(NULL) # Mark invalid datasets as NULL
                }
                cat(sprintf("Structure of dataset at [%d][%d][%d][%d]:\n", i, j, k, l))
                print(str(dataset))
                return(TRUE) # Mark valid datasets
              }))
            } else {
              warning(sprintf("Expected list of lists at [%d][%d][%d], but found: %s", i, j, k, class(innermostlist)))
              return(NULL)
            }
          }))
        } else {
          warning(sprintf("Expected list of lists at [%d][%d], but found: %s", i, j, class(innerlist)))
          return(NULL)
        }
      }))
    } else {
      warning(sprintf("Expected list of lists at index %d, but found: %s", i, class(sublist)))
      return(NULL)
    }
  })
  return(validation_results)
}

# Define the function to fit the models to a dataset
fit_models <- function(data) {
  models <- list()
  if (is_valid_dataset(data)) {
    # Model 1: Linear model with condition*outcome
    model1_data <- data |>
      filter(t_norm > 0 & t_norm < 5000) |>
      group_by(participant_id, condition, outcome, age_cohort) |>
      dplyr::summarise(bc_pupil_size_average = mean(bc_pupil_size, na.rm = TRUE)) |>
      ungroup()
    models$model1 <- tryCatch(lm(bc_pupil_size_average ~ condition * outcome * age_cohort, data = model1_data), error = function(e) e)

    # Model 2: Linear mixed model with condition*outcome across trial time
    models$model2 <- tryCatch(lmer(bc_pupil_size ~ t_norm * condition * outcome * age_cohort + (1 + t | participant_id), data = data), error = function(e) e)

    # Model 3: Generalized additive model with smooth terms
    models$model3 <- tryCatch(
      bam(
        bc_pupil_size ~ condition * outcome * age_cohort +
          s(t_norm, by = lab_id, k = 7) +
          s(t_norm, by = condition, k = 20) +
          s(t_norm, by = outcome, k = 20) +
          s(t_norm, participant_id, bs = "fs", m = 1),
        data = data, discrete = TRUE, nthreads = 40
      ),
      error = function(e) e
    )
  } else {
    models$error <- "Provided data is not a valid data frame."
  }
  return(models)
}

# Model fitting: Process valid datasets in megaverse
fit_models_to_valid_datasets <- function(megaverse, validation_results) {
  model_results <- map2(megaverse, validation_results, function(sublist, valid_sublist) {
    if (is.list(sublist) && !is.null(valid_sublist)) {
      return(map2(sublist, valid_sublist, function(innerlist, valid_innerlist) {
        if (is.list(innerlist) && !is.null(valid_innerlist)) {
          return(map2(innerlist, valid_innerlist, function(innermostlist, valid_innermostlist) {
            if (is.list(innermostlist) && !is.null(valid_innermostlist)) {
              return(map2(innermostlist, valid_innermostlist, function(dataset, is_valid) {
                dataset <- extract_data_frame(dataset)
                if (is.null(is_valid)) {
                  return(list(error = "Invalid data frame. Skipped processing."))
                }
                return(fit_models(dataset))
              }))
            }
            return(NULL)
          }))
        }
        return(NULL)
      }))
    }
    return(NULL)
  })
  return(model_results)
}

# Validate all datasets first
validation_results <- validate_megaverse(megaverse)

# Fit models to only valid datasets
megaverse_model_results <- fit_models_to_valid_datasets(megaverse, validation_results)

# Print the results (or handle them as needed)
cat("Model fitting results:\n")
print(megaverse_model_results)
```
To select the best model based on the lowest BIC, lowest AIC, and highest R-squared (Calignano et al., 2024), we computed the aboved cited statistics for each model and then compare them. We create a function to extract these metrics, identify the best model based on each criterion, and then plot the effects of the best model.

```{r, MODEL SELECTION, echo=TRUE,include=TRUE, out.width='90%'}
# Function to extract BIC, AIC, and R-squared from a model
extract_model_metrics <- function(model) {
  if (inherits(model, "lm") || inherits(model, "lmerMod") || inherits(model, "bam")) {
    aic_value <- AIC(model)
    bic_value <- BIC(model)
    r_squared <- if (inherits(model, "lm")) {
      summary(model)$r.squared
    } else if (inherits(model, "lmerMod")) {
      r.squaredGLMM(model)[1] # Marginal R-squared for mixed models
    } else if (inherits(model, "bam")) {
      summary(model)$r.sq
    } else {
      NA
    }
    return(list(AIC = aic_value, BIC = bic_value, R_squared = r_squared))
  } else {
    return(NULL)
  }
}

# Function to compare models and select the best based on AIC, BIC, and R-squared
compare_models <- function(models_list) {
  model_metrics <- map(models_list, ~ map(.x, extract_model_metrics))

  best_models <- list()
  best_models$AIC <- model_metrics |>
    map_df(~ map_df(.x, ~ data.frame(AIC = .x$AIC)), .id = "model") |>
    arrange(AIC) |>
    slice(1)

  best_models$BIC <- model_metrics |>
    map_df(~ map_df(.x, ~ data.frame(BIC = .x$BIC)), .id = "model") |>
    arrange(BIC) |>
    slice(1)

  best_models$R_squared <- model_metrics |>
    map_df(~ map_df(.x, ~ data.frame(R_squared = .x$R_squared)), .id = "model") |>
    arrange(desc(R_squared)) |>
    slice(1)

  return(best_models)
}

library(ggplot2)

# Function to plot the effects of a model
plot_model_effects <- function(model) {
  if (inherits(model, "lm")) {
    plot_data <- data.frame(
      Fitted = fitted(model),
      Residuals = residuals(model)
    )
    ggplot(plot_data, aes(Fitted, Residuals)) +
      geom_point() +
      geom_smooth(method = "loess") +
      labs(title = "Effects of the Best Linear Model")
  } else if (inherits(model, "lmerMod")) {
    plot_data <- data.frame(
      Fitted = fitted(model),
      Residuals = residuals(model)
    )
    ggplot(plot_data, aes(Fitted, Residuals)) +
      geom_point() +
      geom_smooth(method = "loess") +
      labs(title = "Effects of the Best Linear Mixed Model")
  } else if (inherits(model, "bam")) {
    plot(model, pages = 1, main = "Effects of the Best Generalized Additive Model")
  } else {
    stop("Unknown model type")
  }
}
```
