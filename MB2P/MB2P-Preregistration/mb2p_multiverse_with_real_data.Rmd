---
title: "MB2-P - Multiverse Data Simulation and Statistical Analysis"
author: "Rmarkdown by Giulia Calignano, Marlena Mayer, Robert Hepach "
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    css: styles.css
  pdf_document: default
  word_document: default
  theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(MASS)
library(stringi)
```

## The Data needed

- **participant_id**: A unique identifier for each participant, generated as a random string. Each participant has a unique ID to distinguish them in the dataset.
- **age_cohort**: A randomly generated string intended to categorize participants into different age cohorts.
- **t**: A numeric variable representing a timestamp or duration, generated within a specified range and with a possibility of missing values (NA).
- **x and y**: Numeric variables generated within specified ranges, intended to represent coordinates or other measurements, with a possibility of missing values.
- **pupil_left and pupil_right**: Numeric variables representing measurements of the left and right pupil sizes, respectively. These variables are generated with a specified correlation and then rescaled to a specific range.
- **lab_id**: A randomly generated string serving as a unique identifier for the lab or testing location.
- **conditions**: A categorical variable with two levels, i.e. "knowledge" and "ignorance"
- **outcomes**: A categorical variable with two levels, i.e. "incongruent" and "congruent"

## The Multiverse forking paths of pupillometry preprocessing 

The following script specifies the preprocessing and analysis steps of the multiverse approach applied to the MB2-P (see Calignano, Girardi, and Alto√©, 2023).

Let's assume that we have 10s worth of data per participant which starts at 1s before the resolution until 9s after the resolution. The crucial time-window for our analyses is the 5s after the bear exits the tubes, i.e,. from 1s to 6s in this dataset.

# First Degree of freedom: Filtering of extreme yet plausible pupil values, i.e.<2mm, >8mm  

```{r, DF1 EXTREME TONIC VALUES, echo=TRUE,include=TRUE, out.width='90%'}
# Install and load required packages
if (!require('dplyr')) install.packages('dplyr')
if (!require('tidyr')) install.packages('tidyr')
library(dplyr)
library(tidyr)

set.seed(123)  # For reproducibility

# load pupillometry data
library(here)
source(here('helper','ensure_repo_structure.R'))

load(here(INTERMEDIATE_FOLDER, INTERMEDIATE_008))

# subsetting to Tobii data for now
data_pupillometry <- data_pupillometry %>%
  filter(eyetracker_type=="Tobii")

# excluding Nijmegen data as pupil values are in the hundreds
data_pupillometry <- data_pupillometry %>%
  filter(lab_id!="babylabNijmegen")

# filter to second test trial only if needed
data_pupillometry <- data_pupillometry %>% filter(trial_num==6)

# filter to -1000 to 5000 ms
## MSS: RStudio always crashed for me so I decided to work with a reduced data set
data_pupillometry <- data_pupillometry %>% filter(t_norm>=-1000 & t_norm<5000)

#### plausible value range ]2,8[ - from 1 to 2 DATASETS
data_pupillometry$age_cohort = as.factor(data_pupillometry$age_cohort)
data_pupillometry$average <- (data_pupillometry$pupil_left + data_pupillometry$pupil_right)/2
all <- data_pupillometry
all$step <- "implausible"
summary(all)

plausible <- data_pupillometry |> 
  mutate(average = ifelse(average > 2 | average < 8, NA, average))
# plausible <- data_pupillometry[data_pupillometry$average>2 & data_pupillometry$average<8,] #RH: Does it matter that the number of rows is reduced as opposed to creating NAs and keeping number of rows the same? #GC in terms of computational problems there are no differences 

plausible$step <- "plausible"
summary(plausible)

all_plausible <- list(all, plausible)

all$pupil_left = as.numeric(as.character(all$pupil_left))
all$pupil_right = as.numeric(as.character(all$pupil_right))
all <- all |> 
  mutate(pupil_left = ifelse(pupil_left < 0 | pupil_right < 0, NA, pupil_left),
         pupil_right = ifelse(pupil_left < 0 | pupil_right < 0, NA, pupil_right))
# all = all[all$pupil_left >0 & all$pupil_right>0,] # MSS this currently creates rows with NAs resulting in the next plot to also have NAs as age_cohort facet

all %>% mutate(Color = ifelse(pupil_left <=2 | pupil_left >=8 | pupil_right <=2 | pupil_right >=8,"blue", "red")) %>%
  ggplot(aes(x = pupil_left, y= pupil_right, color = Color))+
  geom_point(alpha= 0.2)+ xlab("pupil size left eye (mm)") + ylab("pupil size right eye (mm)") +
  scale_color_identity() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black")) + facet_wrap("age_cohort")

plot(all$x,all$y)
```
# Second degree of freedom: fixation within the screen vs outside the screen

```{r, DF2 Area Of Interest, echo=TRUE,include=TRUE, out.width='90%'}
AOI_all <- all[all$x > 0,]
AOI_all <- AOI_all[AOI_all$x < 1280,]
AOI_all <- AOI_all[AOI_all$y > 0 ,]
AOI_all <- AOI_all[AOI_all$y < 960,]
AOI_all$step <- "AOI_implausible"
summary(AOI_all)

AOI_plausible <-plausible[plausible$x > 0,]
AOI_plausible <- AOI_plausible[AOI_plausible$x < 1280,]
AOI_plausible <- AOI_plausible[AOI_plausible$y > 0 ,]
AOI_plausible <- AOI_plausible[AOI_plausible$y < 960,]
AOI_plausible$step <- "AOI_plausible"
summary(AOI_plausible)

AOI <- list(AOI_all, AOI_plausible, all, plausible)
plot(AOI_all$x,AOI_all$y)

```
#Third degree of freedom: moving average filtered vs unfiltered data

```{r, DF3 moving average, echo=TRUE,include=TRUE, out.width='90%'}
# Install and load required packages
if (!require('zoo')) install.packages('zoo')
library(zoo)
#empty object
AOI_MA <- AOI
# Function to apply moving average to a single dataset
apply_moving_average <- function(data, window_size = 5) {
  if (!is.data.frame(data)) {
    stop("The provided data is not a data frame")
  }
  data %>%
    group_by(participant_id, Trial) %>%
    mutate(
      pupil_left_MA = zoo::rollapply(pupil_left, width = window_size, FUN = mean, na.rm = TRUE, fill = NA, align = 'right'),
      pupil_right_MA = zoo::rollapply(pupil_right, width = window_size, FUN = mean, na.rm = TRUE, fill = NA, align = 'right')
    ) %>%
    ungroup()
}

# Iterate through the AOI list of lists and apply the moving average function
for (i in seq_along(AOI_MA)) {
  for (j in seq_along(AOI_MA[[i]])) {
    if (is.data.frame(AOI_MA[[i]][[j]])) {
      AOI_MA[[i]][[j]] <- apply_moving_average(AOI[[i]][[j]])
    } 
  }
}

MA <- list(AOI, AOI_MA)

```

# Fourth degree of freedom: 1s, 0.5s, or 0.25s before the bear resolution.

We consider three possible baseline correction all performed by subtracting the average pupil diameter from all subsequent values between, dividing those values by the average baseline, and averaging the baseline-corrected values vector.

```{r, DF4 BASELINE-CORRECTION, echo=TRUE,include=TRUE, out.width='90%'}
### 3 median baselines, i.e., initial time window for each trial_num ,id, lab_id
### i.e. 1s, 0.5s, or 0.25s before resolution.
# MSS According to the current data set, the resolution is at 0 ms for t_norm!
library(plyr)
universe_one <- MA
# 1s baseline 

# MSS Is it ok if we label the baseline-corrected pupil size bc_pupil_size?

           # Iterate through each list within universe_one
for (i in seq_along(universe_one)) {
  # Iterate through each data frame within the current list
  for (j in seq_along(universe_one[[i]])) {
    # Check if the current element is a data frame
    if (is.data.frame(universe_one[[i]][[j]])) {
      # Apply the baseline correction
      universe_one[[i]][[j]] <- ddply(universe_one[[i]][[j]], .(participant_id, condition, outcome, lab_id), transform, bc_pupil_size = {        avg_filtered <- average[t_norm >= -1000 & t_norm < 0]
                                        avg_mean <- mean(avg_filtered, na.rm = TRUE)
                                        average - avg_mean
                                      })
    } else {
      warning(paste("Element universe_one[[", i, "]][[", j, "]] is not a data frame and will be skipped.", sep = ""))
    }
  }
}


#500 ms baseline 
universe_two <- MA
for (i in seq_along(universe_two)) {
  # Iterate through each data frame within the current list
  for (j in seq_along(universe_two[[i]])) {
    # Check if the current element is a data frame
    if (is.data.frame(universe_two[[i]][[j]])) {
      # Apply the baseline correction
      universe_two[[i]][[j]] <- ddply(universe_two[[i]][[j]], .(participant_id, condition, outcome, lab_id), transform, bc_pupil_size = {        avg_filtered <- average[t_norm >= -500 & t_norm < 0]
                                        avg_mean <- mean(avg_filtered, na.rm = TRUE)
                                        average - avg_mean
                                      })
    } else {
      warning(paste("Element universe_one[[", i, "]][[", j, "]] is not a data frame and will be skipped.", sep = ""))
    }
  }
}

#250 ms baseline 
universe_three <- MA
 for (i in seq_along(universe_three)) {
  # Iterate through each data frame within the current list
  for (j in seq_along(universe_three[[i]])) {
    # Check if the current element is a data frame
    if (is.data.frame(universe_three[[i]][[j]])) {
      # Apply the baseline correction
      universe_three[[i]][[j]] <- ddply(universe_three[[i]][[j]], .(participant_id, condition, outcome, lab_id), transform, bc_pupil_size = {        avg_filtered <- average[t_norm >= -250 & t_norm < 0]
                                        avg_mean <- mean(avg_filtered, na.rm = TRUE)
                                        average - avg_mean
                                      })
    } else {
      warning(paste("Element universe_three[[", i, "]][[", j, "]] is not a data frame and will be skipped.", sep = ""))
    }
  }
}

#multiverse
multiverse <- list(universe_one, universe_two, universe_three)

#plot
library(gridExtra)
data_summary <- function(x) {
  m <- median(x)
  ymin <- m-sd(x)
  ymax <- m+sd(x)
  return(c(y=m,ymin=ymin,ymax=ymax))
}

ggplot(multiverse[[2]][[2]][[2]] %>% filter(!is.na(bc_pupil_size)& (t_norm >= 0 & t_norm < 5000)), aes(t_norm, bc_pupil_size,condition, colour =condition)) +
               labs(x = "time (ms)",y = "pupil size (mm)", colour = NULL)+
               geom_vline(xintercept = 0 )+ geom_smooth(se = T) + facet_wrap(outcome ~ age_cohort) + 
               geom_hline(yintercept = 0) +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                                                    panel.background = element_blank(), axis.line = element_line(colour = "black"))
                                               
ggplot(multiverse[[2]][[1]][[1]] %>% filter(!is.na(bc_pupil_size) & (t_norm >= 0 & t_norm < 5000)), aes(t_norm,bc_pupil_size,condition, colour =condition)) +
               labs(x = "time (ms)",y = "pupil size (mm)", colour = NULL)+
               geom_vline(xintercept = 0 )+ geom_smooth(se = T) + facet_wrap(outcome ~ age_cohort) + 
               geom_hline(yintercept = 0) +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                                                    panel.background = element_blank(), axis.line = element_line(colour = "black"))
  
```
At this point, the multiverse object contains a list of three elements, each with a nested structure consisting of four data frames. Each data frame has the same structure but differs in the observations they contain. Here is a breakdown of the structure:

List of 3: The top-level list contains three elements. Each element in this list is itself a list containing four data frames.

Each of the three lists:

Contains four data frames, each having 27 variables (columns).
The structure of these data frames is the same, but they have different numbers of observations (rows).
//
<br>

# Fifth degree of freedom: Participant exclusion (following the criteria of MB2) at the level of the 1st vs 2nd trial. 

Here we use two strings to exclude participants who provided valid data only on the second test trial (remove_ids_1) or who provided valid data on both test trials (remove_ids_2).

```{r, DF5 PARTICIPANT EXCLUSION, echo=TRUE,include=TRUE, out.width='90%'}
# Load necessary library
library(dplyr)

# Function to handle nested lists and remove specific participant_ids
remove_participants <- function(multiverse, participant_ids_to_remove) {

  clean_data <- function(data) {
    # Check if the element is a data frame
    if (is.data.frame(data)) {
      return(data %>% filter(!participant_id %in% participant_ids_to_remove))
    }
    # If it's a list, apply the function recursively
    else if (is.list(data)) {
      return(lapply(data, clean_data))
    }
    # If it's neither, return it unchanged (or NULL if you prefer skipping non-list, non-data.frame items)
    else {
      return(NULL)
    }
  }

  # Apply the cleaning function at the top level
  cleaned_multiverse <- lapply(multiverse, clean_data)

  return(cleaned_multiverse)
}

# Define participant IDs to remove for the second and third multiverse
remove_ids_1 <- unique(data_pupillometry$participant_lab_id[data_pupillometry$valid_second_test_trial == 1])
remove_ids_2 <- unique(data_pupillometry$participant_lab_id[data_pupillometry$valid_first_test_trial == 1 & data_pupillometry$valid_second_test_trial == 1])

# Create the three multiverses
multiverse_one <- multiverse
multiverse_two <- remove_participants(multiverse, remove_ids_1)
multiverse_three <- remove_participants(multiverse, remove_ids_2)

# Combine into the megaverse
megaverse <- list(multiverse_one, multiverse_two, multiverse_three)

```
At this point we have an object called megaverse. The megaverse is a comprehensive collection of datasets organized into three different versions, each known as a multiverse. Each multiverse represents a different scenario or variation of the original dataset. Here's a detailed description of the structure:

Original Multiverse (multiverse_one): This version contains the unaltered original data. It serves as the baseline for comparison with other multiverses.
Modified Multiverse 1 (multiverse_two): In this version, certain participants are removed based on a specified list of participant IDs (remove_ids_1). This allows for analyzing how the removal of specific participants affects the results.
Modified Multiverse 2 (multiverse_three): This version excludes a different set of participants, defined by another list of participant IDs (remove_ids_2). This provides another perspective on the dataset with a different subset of participants removed.
Each multiverse consists of:

A list of lists, where each sublist contains one or more data frames.
Each data frame represents a segment of the dataset, including columns such as participant_id and other relevant variables included in the simulated data.
By structuring the data in this way, the megaverse allows for systematic comparison across different scenarios, facilitating a robust analysis of the dataset under various conditions.

<br>

As a final step, we investigated (1) the averaged interaction effect Condition x Outcome, (2) the time-course of the interaction effect Condition x Outcome and (3) the non linear interaction effect of Condition x Outcome considering the time-course of the effect. This approach provided an exploration of whether and how smoothing time enhances the plausibility of statistical modeling of pupil dilation across the datasets we created.

For the analysis of pupillary data, we utilized generalized additive mixed modeling (GAMM, Wood, 2011). GAMMs combine the flexibility of generalized additive models (GAMs) with the ability to incorporate random effects, which are essential for accounting for correlations among observations within clusters or groups. This includes the use of smooth functions that handle both continuous and categorical predictors. The random effects component facilitates the inclusion of hierarchical structures, such as nested or repeated measures within the data, making GAMMs useful for a wide range of data types, including time series, spatial, and longitudinal data. The model is estimated using penalized regression techniques, which help to avoid overfitting and produce more reliable predictions.

```{r, SETTING MODEL, FUNCTIONS, echo=TRUE,include=TRUE, out.width='90%'}
# Inspect the structure of megaverse
str(megaverse, max.level = 3)

extract_data_frame <- function(data) {
  while (is.list(data) && length(data) == 1) {
    data <- data[[1]]
  }
  return(data)
}

library(dplyr)
library(lme4)
library(mgcv)
library(purrr)

# Function to check if all dataset are a data frame
is_valid_dataset <- function(data) {
  return(is.data.frame(data))
}

# Pre-validation: Validate all datasets in megaverse with debug information
validate_megaverse <- function(megaverse) {
  validation_results <- map2(megaverse, seq_along(megaverse), function(sublist, i) {
    if (is.list(sublist)) {
      return(map2(sublist, seq_along(sublist), function(innerlist, j) {
        if (is.list(innerlist)) {
          return(map2(innerlist, seq_along(innerlist), function(innermostlist, k) {
            if (is.list(innermostlist)) {
              return(map2(innermostlist, seq_along(innermostlist), function(dataset, l) {
                dataset <- extract_data_frame(dataset)
                cat(sprintf("Validating dataset at [%d][%d][%d][%d]: Type = %s\n", i, j, k, l, class(dataset)))
                if (!is_valid_dataset(dataset)) {
                  warning(sprintf("Dataset at [%d][%d][%d][%d] is not a valid data frame. Type: %s", i, j, k, l, class(dataset)))
                  return(NULL)  # Mark invalid datasets as NULL
                }
                cat(sprintf("Structure of dataset at [%d][%d][%d][%d]:\n", i, j, k, l))
                print(str(dataset))
                return(TRUE)  # Mark valid datasets
              }))
            } else {
              warning(sprintf("Expected list of lists at [%d][%d][%d], but found: %s", i, j, k, class(innermostlist)))
              return(NULL)
            }
          }))
        } else {
          warning(sprintf("Expected list of lists at [%d][%d], but found: %s", i, j, class(innerlist)))
          return(NULL)
        }
      }))
    } else {
      warning(sprintf("Expected list of lists at index %d, but found: %s", i, class(sublist)))
      return(NULL)
    }
  })
  return(validation_results)
}

# Define the function to fit the models to a dataset
fit_models <- function(data) {
  models <- list()
  if (is_valid_dataset(data)) {
    # Model 1: Linear model with condition*outcome
    model1_data <- data %>%
      filter(t_norm > 0 & t_norm < 5000) %>%
      group_by(participant_id, condition, outcome, age_cohort) %>%
      dplyr::summarise(bc_pupil_size_average = mean(bc_pupil_size, na.rm = TRUE)) %>%
      ungroup()
    models$model1 <- tryCatch(lm(bc_pupil_size_average ~ condition * outcome * age_cohort, data = model1_data), error = function(e) e)

    # Model 2: Linear mixed model with condition*outcome across trial time
    models$model2 <- tryCatch(lmer(bc_pupil_size ~ t_norm * condition * outcome * age_cohort + (1 + t | participant_id), data = data), error = function(e) e)

    # Model 3: Generalized additive model with smooth terms
    models$model3 <- tryCatch(
      bam(bc_pupil_size ~ condition * outcome * age_cohort + 
      s(t_norm, by = lab_id, k=7) +
      s(t_norm, by = condition, k=20) +
      s(t_norm, by = outcome, k=20) + 
      s(t_norm, participant_id, bs = 'fs', m = 1), 
      data = data, discrete = TRUE, nthreads = 40), 
      error = function(e) e
    )
  } else {
    models$error <- "Provided data is not a valid data frame."
  }
  return(models)
}

# Model fitting: Process valid datasets in megaverse
fit_models_to_valid_datasets <- function(megaverse, validation_results) {
  model_results <- map2(megaverse, validation_results, function(sublist, valid_sublist) {
    if (is.list(sublist) && !is.null(valid_sublist)) {
      return(map2(sublist, valid_sublist, function(innerlist, valid_innerlist) {
        if (is.list(innerlist) && !is.null(valid_innerlist)) {
          return(map2(innerlist, valid_innerlist, function(innermostlist, valid_innermostlist) {
            if (is.list(innermostlist) && !is.null(valid_innermostlist)) {
              return(map2(innermostlist, valid_innermostlist, function(dataset, is_valid) {
                dataset <- extract_data_frame(dataset)
                if (is.null(is_valid)) {
                  return(list(error = "Invalid data frame. Skipped processing."))
                }
                return(fit_models(dataset))
              }))
            }
            return(NULL)
          }))
        }
        return(NULL)
      }))
    }
    return(NULL)
  })
  return(model_results)
}

# Validate all datasets first
validation_results <- validate_megaverse(megaverse)

# Fit models to only valid datasets
megaverse_model_results <- fit_models_to_valid_datasets(megaverse, validation_results)

# Print the results (or handle them as needed)
cat("Model fitting results:\n")
print(megaverse_model_results)


```
To select the best model based on the lowest BIC, lowest AIC, and highest R-squared (Calignano et al., 2024), we computed the aboved cited statistics for each model and then compare them. We create a function to extract these metrics, identify the best model based on each criterion, and then plot the effects of the best model.

```{r, MODEL SELECTION, echo=TRUE,include=TRUE, out.width='90%'}
# Function to extract BIC, AIC, and R-squared from a model
extract_model_metrics <- function(model) {
  if (inherits(model, "lm") || inherits(model, "lmerMod") || inherits(model, "bam")) {
    aic_value <- AIC(model)
    bic_value <- BIC(model)
    r_squared <- if (inherits(model, "lm")) {
      summary(model)$r.squared
    } else if (inherits(model, "lmerMod")) {
      r.squaredGLMM(model)[1]  # Marginal R-squared for mixed models
    } else if (inherits(model, "bam")) {
      summary(model)$r.sq
    } else {
      NA
    }
    return(list(AIC = aic_value, BIC = bic_value, R_squared = r_squared))
  } else {
    return(NULL)
  }
}

# Function to compare models and select the best based on AIC, BIC, and R-squared
compare_models <- function(models_list) {
  model_metrics <- map(models_list, ~map(.x, extract_model_metrics))
  
  best_models <- list()
  best_models$AIC <- model_metrics %>%
    map_df(~map_df(.x, ~data.frame(AIC = .x$AIC)), .id = "model") %>%
    arrange(AIC) %>%
    slice(1)
  
  best_models$BIC <- model_metrics %>%
    map_df(~map_df(.x, ~data.frame(BIC = .x$BIC)), .id = "model") %>%
    arrange(BIC) %>%
    slice(1)
  
  best_models$R_squared <- model_metrics %>%
    map_df(~map_df(.x, ~data.frame(R_squared = .x$R_squared)), .id = "model") %>%
    arrange(desc(R_squared)) %>%
    slice(1)
  
  return(best_models)
}

library(ggplot2)

# Function to plot the effects of a model
plot_model_effects <- function(model) {
  if (inherits(model, "lm")) {
    plot_data <- data.frame(
      Fitted = fitted(model),
      Residuals = residuals(model)
    )
    ggplot(plot_data, aes(Fitted, Residuals)) +
      geom_point() +
      geom_smooth(method = "loess") +
      labs(title = "Effects of the Best Linear Model")
  } else if (inherits(model, "lmerMod")) {
    plot_data <- data.frame(
      Fitted = fitted(model),
      Residuals = residuals(model)
    )
    ggplot(plot_data, aes(Fitted, Residuals)) +
      geom_point() +
      geom_smooth(method = "loess") +
      labs(title = "Effects of the Best Linear Mixed Model")
  } else if (inherits(model, "bam")) {
    plot(model, pages = 1, main = "Effects of the Best Generalized Additive Model")
  } else {
    stop("Unknown model type")
  }
}


```
